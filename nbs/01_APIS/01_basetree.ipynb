{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp base.tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaseTree\n",
    "\n",
    "> More on this soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.utils import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mltreelib.utils.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# todo: Add followings\n",
    "# 1) Parallelization strategy\n",
    "# 1) Monotone constraints\n",
    "# 1) All the provided data constraints\n",
    "# 1) How to enable randomization for Random forest and ExtraTree Regressors\n",
    "# 1) Cost complexity tables\n",
    "# 1) Implement Survival / Anova / Poission / Oblique / Functional /Model based partitions / Conditional inference trees \n",
    "# 1) Implement crossvlaidatoin as control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "\n",
    "class Node:\n",
    "    \"\"\" A generics node for all tress\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                nodeid:int = None,     # As simple id of the node\n",
    "                depth: int = None,       # depth of current node\n",
    "                split: Union[str,int,list] = None,     # Split definitions, New implementation coming\n",
    "                kids: Union[None, list] = None,  # New implementation coming\n",
    "                surrogates: Union[None, list] = None,  # New implementation coming\n",
    "                n_subnodes: int = 2,  # Default for CART behaviors, if None then would assume C4.5 / ID3 behavior\n",
    "                info:str = None   # just a generic info about the node\n",
    "                ) -> None:\n",
    "\n",
    "        self.nodeid, self.split, self.kids, self.surrogates = nodeid, split, kids, surrogates\n",
    "        self.n_subnodes, self.depth, self.addons = n_subnodes, depth, info  # added information needed for diagnosis\n",
    "        self._estimator, self._loss, self._wt, self._grad, self._cp, self._error =  None, None, None, None, None, None\n",
    "\n",
    "        self._validate()\n",
    "\n",
    "    def _validate(self):\n",
    "        ...\n",
    "\n",
    "    def __str__(self):\n",
    "\n",
    "        return f\"Node(ID:{self.nodeid}, depth:{self.depth}, nchilds:{len(self.kids)}, surrogates:{len(self.surrogates)}, info:{self.info})\"\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "\n",
    "class DecisionNode():\n",
    "    \"\"\"Class that represents a decision node or leaf in the decision tree\"\"\"\n",
    "    def __init__(self, \n",
    "                \n",
    "                feature_i: Union[str,int]=None,  # Feature index which we want to use as the threshold measure.\n",
    "                threshold: float =None,          # The value that we will compare feature values at feature_i against to determine the prediction.\n",
    "                value=None,                      # The class prediction if classification tree, or float value if regression tree.\n",
    "                true_branch=None,                # Next decision node for samples where features value met the threshold.\n",
    "                false_branch=None,               # Next decision node for samples where features value did not meet the threshold.\n",
    "                ):\n",
    "        self.feature_i = feature_i    \n",
    "        self.threshold= threshold             \n",
    "        self.value = value                            #\n",
    "        self.true_branch = true_branch  #\n",
    "        self.false_branch = false_branch# \n",
    "        self.estimator =  None                        # An estimator that can run at leaf node \n",
    "        self.is_leaf = False                          # indicator weather current leaf is final leaf or not\n",
    "        self.loss = None                              # loss of gradient boosting\n",
    "        self.grad = None                              # gradient\n",
    "        self.w = None                                 # weight\n",
    "        self.resid = None                             # Residuals\n",
    "     \n",
    "        \n",
    "    def estimate(self,x):\n",
    "        return self.value if self.estimator is None else self.estimator.predict(x)\n",
    "\n",
    "# Super class of RegressionTree and ClassificationTree\n",
    "class BaseDecisionTree(object):\n",
    "    \"\"\"Super class of all kinds of tree.\"\"\"\n",
    "    def __init__(self, \n",
    "                objective:str = \"class\",   # defining objective of the whole tree building.\n",
    "                min_samples_split:int = 20,   # The minimum number of samples needed to make a split when building a tree.\n",
    "                min_sample_leaf: int = 10,    # Minimum sample required to have a leaf node\n",
    "                max_compete: int = 4,         # the number of competitor splits retained in the output. It is useful to know not just which split was chosen, but which variable came in second, third, etc.\n",
    "                min_impurity:float = 0.01,    # The minimum impurity required to split the tree further. this is equivalent to complexity in CART\n",
    "                max_depth:int = float(\"inf\"), # The maximum depth of a tree.\n",
    "                max_surrogates: int = 2,         # more on this to make it generalized and not CART Specific\n",
    "                surrogate_style: int = 2,        # more on this to make it generalized and not CART Specific\n",
    "                parallelize: str = 'feature',    # follow form LightGBMs behavior\n",
    "                tree_growth: str = 'cart',   # follow binary structure if cart else follow multiple structure as C4.5\n",
    "                feature_weights: Union[float,int,list] = None, # weight of each feature in the split. Default is set to 1 for all features\n",
    "                loss: Union[str, float] = None,      # String of loss or function which defines the loss. This amounts to Loss function that is used for Gradient Boosting models to calculate impurity.\n",
    "                verbose:Union[bool,int]=False, # Verbosity for tree building\n",
    "                digits:Union[int, None] = 2    # To round the values before doing a split \n",
    "                ):\n",
    "\n",
    "\n",
    "\n",
    "        self.root = None  # Root node in dec. tree\n",
    "        self.min_samples_split, self.min_sample_leaf = min_samples_split ,min_sample_leaf\n",
    "        self.max_compete, self.min_impurity, self.max_depth  = max_compete, min_impurity, max_depth\n",
    "        self.max_surrogates, self.surrogate_style = max_surrogates ,surrogate_style\n",
    "        self.tree_growth, self.feature_weights = tree_growth, feature_weights\n",
    "\n",
    "        # function defining how tree would grow\n",
    "        self._node_function = None\n",
    "        self._leaf_function = None\n",
    "        self._split_function = None\n",
    "        self._summary_function = None\n",
    "        self._parallelization = parallelize\n",
    "        \n",
    "\n",
    "        # If y is one-hot encoded (multi-dim) or not (one-dim)\n",
    "        self.one_dim = None\n",
    "\n",
    "        # If Gradient Boost\n",
    "        self._objective = objective\n",
    "        self._loss = loss\n",
    "\n",
    "        self.cat_features = None\n",
    "        self.con_features = None\n",
    "        self.features_ = None\n",
    "        self.target_ = None\n",
    "\n",
    "        # other things\n",
    "        self.digits, self.verbose  = digits, verbose\n",
    "\n",
    "        self._validate_params()\n",
    "\n",
    "    def _validate_params(self):\n",
    "        \n",
    "        if self.loss not in ['mse','mae','rss', 'gini', 'entropy','friedman_mse','chisq','anova','var','std']:\n",
    "            raise ValueError(f\"{self.loss} is not a known loss\")\n",
    "        # todo : add other validations\n",
    "\n",
    "\n",
    "    def fit(self, X, y, loss=None):\n",
    "        \"\"\" Build decision tree \"\"\"\n",
    "        self.cat_features = X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "        self.con_features = [col for col in X.columns if col not in self.cat_features]\n",
    "        self.features_ = X.columns.tolist()\n",
    "        self.target_ = y.name\n",
    "        self.one_dim = len(np.shape(y)) == 1\n",
    "        # # Check if expansion of y is needed\n",
    "        # if len(np.shape(y)) == 1:\n",
    "        #     y = np.expand_dims(y, axis=1)\n",
    "\n",
    "        # Add y as last column of X\n",
    "        Xy = pd.concat((X, y), axis=1)\n",
    "        self.root = self._build_tree(Xy)\n",
    "        self.loss=None\n",
    "\n",
    "    def _build_tree(self, Xy, current_depth=0,features=None):\n",
    "        \"\"\" Recursive method which builds out the decision tree and splits X and respective y\n",
    "        on the feature of X which (based on impurity) best separates the data\"\"\"\n",
    "\n",
    "        largest_impurity = 0\n",
    "        bst_estimator = None    \n",
    "        bst_weights = None        \n",
    "        best_criteria = {\"feature_i\": None, \"threshold\": None} # Feature index and threshold\n",
    "        best_sets = {\n",
    "            \"leftXy\": None,   # Xy of left subtree\n",
    "            \"rightXy\": None # Xy of right subtree\n",
    "            }# Subsets of the data\n",
    "        \n",
    "        n_samples, n_features = np.shape(Xy)\n",
    "        if len(self.features_) + 1  == n_features:\n",
    "            if self.verbose : print(f\"\\nWe are going good at {current_depth} depth\")\n",
    "        else:\n",
    "            print('Something is wrong',n_samples, n_features, self.features_)\n",
    "\n",
    "        if n_samples >= self.min_samples_split and current_depth <= self.max_depth:\n",
    "            # Calculate the impurity for each feature\n",
    "            for feature_i in self.features_:\n",
    "                if self.verbose : print(f'Evaluating {feature_i}')\n",
    "                # All values of feature_i       \n",
    "                if feature_i in self.cat_features:\n",
    "                    unique_values = get_sorted_cats(Xy[feature_i].values)\n",
    "                    thresh = []\n",
    "                else:\n",
    "                    if self.digits is not None:\n",
    "                        unique_values = np.unique(np.round(Xy[feature_i],self.digits))\n",
    "                    else:\n",
    "                        unique_values = np.unique(Xy[feature_i])\n",
    "                    \n",
    "                # Iterate through all unique values of feature column i and\n",
    "                # calculate the impurity\n",
    "                for threshold in unique_values:\n",
    "                    # Divide X and y depending on if the feature value of X at index feature_i\n",
    "                    # meets the threshold\n",
    "                    if feature_i in self.cat_features:\n",
    "                        thresh.append(threshold)\n",
    "                        Xy1, Xy2 = divide_on_feature(Xy, feature_i, thresh)  \n",
    "                    else:\n",
    "                        Xy1, Xy2 = divide_on_feature(Xy, feature_i, threshold)\n",
    "\n",
    "                    if len(Xy1) > 0 and len(Xy2) > 0:\n",
    "                        \n",
    "                        # Select the y-values of the two sets\n",
    "                        # y1 = Xy1[:, n_features:]\n",
    "                        # y2 = Xy2[:, n_features:]\n",
    "                \n",
    "                        # Calculate impurity\n",
    "                        # impurity = self._impurity_calculation(y, y1, y2)\n",
    "                        \n",
    "                        # evaluate node function\n",
    "                        impurity, estimator, weights, addons = self._node_function(Xy, Xy1, Xy2, target=self.target_,features=self.features_)\n",
    "\n",
    "                        # If this threshold resulted in a higher information gain than previously\n",
    "                        # recorded save the threshold value and the feature\n",
    "                        # index\n",
    "                        if impurity > largest_impurity:\n",
    "                            largest_impurity, bst_estimator, bst_weights, bst_addons = impurity, estimator, weights, addons\n",
    "                            best_criteria[\"feature_i\"]= feature_i\n",
    "                            best_criteria[\"threshold\"] =  thresh if feature_i in self.cat_features else threshold\n",
    "                            best_sets[\"leftXy\"]=  Xy1 # X of left subtree\n",
    "                            best_sets[\"rightXy\"]=  Xy2 # X of left subtree\n",
    "                            \n",
    "                if self.verbose: print(f\"Impurity Update at {best_criteria} with {largest_impurity}\")\n",
    "                            \n",
    "                                \n",
    "        if largest_impurity > self.min_impurity:\n",
    "            if self.verbose: print(f'Best Impurity is with {best_criteria} with {largest_impurity} Split sizes: Left: {best_sets[\"leftXy\"].shape} : Right {best_sets[\"rightXy\"].shape}')\n",
    "            # Build subtrees for the right and left branches\n",
    "            true_branch = self._build_tree(best_sets[\"leftXy\"], current_depth + 1)\n",
    "            false_branch = self._build_tree(best_sets[\"rightXy\"], current_depth + 1)\n",
    "            node = DecisionNode(feature_i=best_criteria[\"feature_i\"], threshold=best_criteria[\n",
    "                                \"threshold\"], true_branch=true_branch, false_branch=false_branch,depth=current_depth)\n",
    "            node.estimator = bst_estimator\n",
    "            node.w = bst_weights\n",
    "            node.addons = bst_addons\n",
    "            return node\n",
    "        else:\n",
    "            if self.verbose: print(f\"No Impurtiy update. Hence a leaf with {best_criteria} with {largest_impurity}\")\n",
    "\n",
    "        # We're at leaf => determine value\n",
    "        leaf_value,estimator,weights,addons = self._leaf_function(Xy, target = self.target_, features=self.features_)\n",
    "        leaf_node = DecisionNode(value=leaf_value)\n",
    "        leaf_node.estimator = estimator\n",
    "        leaf_node.w = weights\n",
    "        leaf_node.addons = addons\n",
    "        return leaf_node\n",
    "\n",
    "\n",
    "    def predict_value(self, x, tree=None,verbose=False):\n",
    "        \"\"\" Do a recursive search down the tree and make a prediction of the data sample by the\n",
    "            value of the leaf that we end up at \"\"\"\n",
    "\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "\n",
    "        # If we have a value (i.e we're at a leaf) => return value as the prediction\n",
    "        if tree.value is not None:\n",
    "            return tree.estimate(x)\n",
    "\n",
    "        # Choose the feature that we will test\n",
    "        if isinstance(x, pd.Series):\n",
    "            feature_value = x.loc[tree.feature_i]\n",
    "        else:\n",
    "            feature_value = x[tree.feature_i].values[0]\n",
    "        if verbose:\n",
    "            print(f'Feature value for {tree.feature_i} :{feature_value}')\n",
    "\n",
    "        # Determine if we will follow left or right branch\n",
    "        branch = tree.false_branch\n",
    "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "            if feature_value >= tree.threshold:  branch = tree.true_branch\n",
    "        elif isinstance(tree.threshold,list):\n",
    "            if feature_value in tree.threshold: branch = tree.true_branch\n",
    "        elif feature_value == tree.threshold: \n",
    "            branch = tree.true_branch\n",
    "\n",
    "        # Test subtree\n",
    "        return self.predict_value(x, branch,verbose)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Classify samples one by one and return the set of labels \"\"\"\n",
    "        y_pred = [self.predict_value(sample) for sample in X]\n",
    "        return y_pred\n",
    "\n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        \"\"\" Recursively print the decision tree \"\"\"\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        # If we're at leaf => print the label\n",
    "        if tree.value is not None:\n",
    "            print (tree.value)\n",
    "        # Go deeper down the tree\n",
    "        else:\n",
    "            # Print test\n",
    "            print (\"%s:%s? \" % (tree.feature_i, tree.threshold))\n",
    "            # Print the true scenario\n",
    "            print (\"%sT->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.true_branch, indent + indent)\n",
    "            # Print the false scenario\n",
    "            print (\"%sF->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.false_branch, indent + indent)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "_objectives = {'class':None,'reg':None,'survival':None,'poission':None}\n",
    "_losses = {'gini':calculate_gini,\n",
    "           'entropy':calculate_entropy,\n",
    "           'std':calculate_std_dev,\n",
    "           'var':calculate_variance,'mse':calculate_mse,\n",
    "           'mae':calculate_mae,'friedman_mse':calculate_mse}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "def calculate_friedman_mse(y_true,y_pred):\n",
    "    \"\"\"Mean squared error impurity criterion with improvement score by Friedman.\n",
    "    Uses the formula (35) in Friedman's original Gradient Boosting paper:\n",
    "        diff = mean_left - mean_right\n",
    "        improvement = n_left * n_right * diff^2 / (n_left + n_right)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \"\"\"Mean squared error impurity criterion.\n",
    "        MSE = var_left + var_right\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class CART(BaseDecisionTree):\n",
    "    \n",
    "    def _calculate_variance_reduction(self, y, y_l, y_r):\n",
    "        \n",
    "        #getting shapes\n",
    "        n, n_l, n_r = len(y), len(y_l), len(y_r)\n",
    "        \n",
    "        if self._loss not in ['friedman_mse','chisq']:\n",
    "            # defining loss function\n",
    "            loss = _losses[self._loss]\n",
    "            \n",
    "            # evaluating losses\n",
    "            parent_loss = loss(y)\n",
    "            e_l, e_r = loss(y_l), loss(y_r)\n",
    "\n",
    "            # impurity gain is difference in loss before vs. after split\n",
    "\n",
    "            if self._loss in ['gini','entropy']:\n",
    "                child_loss = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "                gain = np.sum(parent_loss - child_loss)\n",
    "            elif self._loss in ['rss','mse','mae','std','var']:\n",
    "                gain = e_l + el_r\n",
    "\n",
    "        elif self._loss == 'friedman_mse':\n",
    "            diff = np.mean(y_l) - np.mean(y_r)\n",
    "            gain = n_l*n_r*diff*diff/(n_l+n_r)\n",
    "        \n",
    "\n",
    "        return gain\n",
    "\n",
    "    def _mean_of_y(self, y):\n",
    "        value = np.mean(y, axis=0)\n",
    "        if self.verbose: print('Leaf values:',value,value.shape,len(value.shape))\n",
    "        return value\n",
    "        # return value if len(value.shape) > 1 else value[0]\n",
    "    \n",
    "    def _cart_node_function(self,Xy, Xy1, Xy2, target, features):\n",
    "        y = Xy[[target]].values\n",
    "        y1 = Xy1[[target]].values\n",
    "        y2 = Xy2[[target]].values\n",
    "        impurity = self._calculate_variance_reduction(y,y1,y2)\n",
    "        return impurity,None, None, None\n",
    "    \n",
    "    def _cart_leaf_function(self, Xy, target, features):\n",
    "\n",
    "        if self._objective == 'reg': \n",
    "            value = np.mean(Xy[target].values, axis=0)\n",
    "            if self.verbose: print('Leaf values:',value,value.shape,len(value.shape))\n",
    "            return value, None, None, None\n",
    "        elif self._objective == 'class':\n",
    "            values,counts = np.unique(Xy[target].values,return_counts=True)\n",
    "            counts = counts/np.sum(counts)\n",
    "            return values[np.argmax(counts)],None,{k:v for k,v in zip(values,counts)},None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # output of both node function and leaf function is in form leaf_value/impurity,estimator,weights,addons with inputs\n",
    "        #inputs _node_function(Xy, Xy1,Xy2,target,features)\n",
    "        self._node_function = self._cart_node_function\n",
    "        #inputs _leaf_function(Xy, target, features)\n",
    "        self._leaf_function = self._cart_leaf_function\n",
    "        super(CARTRegressionTree, self).fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class C45(BaseDecisionTree):\n",
    "    def _calculate_information_gain(self, y, y1, y2):\n",
    "        # Calculate information gain\n",
    "        p = len(y1) / len(y)\n",
    "        entropy = calculate_entropy(y)\n",
    "        info_gain = entropy - p * \\\n",
    "            calculate_entropy(y1) - (1 - p) * \\\n",
    "            calculate_entropy(y2)\n",
    "\n",
    "        return info_gain\n",
    "\n",
    "    def _majority_vote(self, y):\n",
    "        most_common = None\n",
    "        max_count = 0\n",
    "        for label in np.unique(y):\n",
    "            # Count number of occurences of samples with label\n",
    "            count = len(y[y == label])\n",
    "            if count > max_count:\n",
    "                most_common = label\n",
    "                max_count = count\n",
    "        return most_common\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self._impurity_calculation = self._calculate_information_gain\n",
    "        self._leaf_value_calculation = self._majority_vote\n",
    "        super(C45ClassificationTree, self).fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mltreedev')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
