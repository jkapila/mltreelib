{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp base.tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "skip_showdoc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "from fastcore.utils import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mltreelib.utils.functions import *\n",
    "from mltreelib.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "# todo: Add followings\n",
    "# 1) Parallelization strategy\n",
    "# 1) Monotone constraints\n",
    "# 1) All the provided data constraints\n",
    "# 1) How to enable randomization for Random forest and ExtraTree Regressors\n",
    "# 1) Cost complexity tables\n",
    "# 1) Implement Survival / Anova / Poission / Oblique / Functional /Model based partitions / Conditional inference trees \n",
    "# 1) Implement crossvlaidatoin as control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "class Node:\n",
    "    \"\"\" A generics node for all tress\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                nodeid:int = None,     # As simple id of the node\n",
    "                depth: int = None,       # depth of current node\n",
    "                split: Union[str,int,list] = None,     # Split definitions, New implementation coming\n",
    "                kids: Union[None, list] = None,  # New implementation coming\n",
    "                surrogates: Union[None, list] = None,  # New implementation coming\n",
    "                n_subnodes: int = 2,  # Default for CART behaviors, if None then would assume C4.5 / ID3 behavior\n",
    "                info:str = None   # just a generic info about the node\n",
    "                ) -> None:\n",
    "\n",
    "        self.nodeid, self.split, self.kids, self.surrogates = nodeid, split, kids, surrogates\n",
    "        self.n_subnodes, self.depth, self.addons = n_subnodes, depth, info  # added information needed for diagnosis\n",
    "        self._estimator, self._loss, self._wt, self._grad, self._cp, self._error =  None, None, None, None, None, None\n",
    "\n",
    "        self._validate()\n",
    "\n",
    "    def _validate(self):\n",
    "        ...\n",
    "\n",
    "    def __str__(self):\n",
    "\n",
    "        return f\"Node(ID:{self.nodeid}, depth:{self.depth}, nchilds:{len(self.kids)}, surrogates:{len(self.surrogates)}, info:{self.info})\"\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "\n",
    "class DecisionNode():\n",
    "    \"\"\"Class that represents a decision node or leaf in the decision tree\"\"\"\n",
    "    def __init__(self, \n",
    "                \n",
    "                feature_i: Union[str,int]=None,  # Feature index which we want to use as the threshold measure.\n",
    "                threshold: float =None,          # The value that we will compare feature values at feature_i against to determine the prediction.\n",
    "                value=None,                      # The class prediction if classification tree, or float value if regression tree.\n",
    "                true_branch=None,                # Next decision node for samples where features value met the threshold.\n",
    "                false_branch=None,               # Next decision node for samples where features value did not meet the threshold.\n",
    "                ):\n",
    "        self.feature_i = feature_i    \n",
    "        self.threshold= threshold             \n",
    "        self.value = value                            #\n",
    "        self.true_branch = true_branch  #\n",
    "        self.false_branch = false_branch# \n",
    "        self.estimator =  None                        # An estimator that can run at leaf node \n",
    "        self.is_leaf = False                          # indicator weather current leaf is final leaf or not\n",
    "        self.loss = None                              # loss of gradient boosting\n",
    "        self.grad = None                              # gradient\n",
    "        self.w = None                                 # weight\n",
    "        self.resid = None                             # Residuals\n",
    "     \n",
    "        \n",
    "    def estimate(self,x):\n",
    "        return self.value if self.estimator is None else self.estimator.predict(x)\n",
    "\n",
    "# Super class of RegressionTree and ClassificationTree\n",
    "class BaseDecisionTree(object):\n",
    "    \"\"\"Super class of all kinds of tree.\"\"\"\n",
    "    def __init__(self, \n",
    "                objective:str = \"class\",   # defining objective of the whole tree building.\n",
    "                min_samples_split:int = 20,   # The minimum number of samples needed to make a split when building a tree.\n",
    "                min_sample_leaf: int = 10,    # Minimum sample required to have a leaf node\n",
    "                max_compete: int = 4,         # the number of competitor splits retained in the output. It is useful to know not just which split was chosen, but which variable came in second, third, etc.\n",
    "                min_impurity:float = 0.01,    # The minimum impurity required to split the tree further. this is equivalent to complexity in CART\n",
    "                max_depth:int = float(\"inf\"), # The maximum depth of a tree.\n",
    "                max_surrogates: int = 2,         # more on this to make it generalized and not CART Specific\n",
    "                surrogate_style: int = 2,        # more on this to make it generalized and not CART Specific\n",
    "                parallelize: str = 'feature',    # follow form LightGBMs behavior\n",
    "                tree_growth: str = 'cart',   # follow binary structure if cart else follow multiple structure as C4.5\n",
    "                feature_weights: Union[float,int,list] = None, # weight of each feature in the split. Default is set to 1 for all features\n",
    "                loss: Union[str, float] = None,      # String of loss or function which defines the loss. This amounts to Loss function that is used for Gradient Boosting models to calculate impurity.\n",
    "                verbose:Union[bool,int]=False, # Verbosity for tree building\n",
    "                digits:Union[int, None] = 2    # To round the values before doing a split \n",
    "                ):\n",
    "\n",
    "\n",
    "\n",
    "        self.root = None  # Root node in dec. tree\n",
    "        self.min_samples_split, self.min_sample_leaf = min_samples_split ,min_sample_leaf\n",
    "        self.max_compete, self.min_impurity, self.max_depth  = max_compete, min_impurity, max_depth\n",
    "        self.max_surrogates, self.surrogate_style = max_surrogates ,surrogate_style\n",
    "        self.tree_growth, self.feature_weights = tree_growth, feature_weights\n",
    "\n",
    "        # function defining how tree would grow\n",
    "        self._node_function = None\n",
    "        self._leaf_function = None\n",
    "        self._split_function = None\n",
    "        self._summary_function = None\n",
    "        self._parallelization = parallelize\n",
    "        \n",
    "\n",
    "        # If y is one-hot encoded (multi-dim) or not (one-dim)\n",
    "        self.one_dim = None\n",
    "\n",
    "        # If Gradient Boost\n",
    "        self._objective = objective\n",
    "        self._loss = loss\n",
    "\n",
    "        self.cat_features = None\n",
    "        self.con_features = None\n",
    "        self.features_ = None\n",
    "        self.target_ = None\n",
    "\n",
    "        # other things\n",
    "        self.digits, self.verbose  = digits, verbose\n",
    "\n",
    "        self._validate_params()\n",
    "\n",
    "    def _validate_params(self):\n",
    "        \n",
    "        if self.loss not in ['mse','mae','rss', 'gini', 'entropy','friedman_mse','chisq','anova','var','std']:\n",
    "            raise ValueError(f\"{self.loss} is not a known loss\")\n",
    "        # todo : add other validations\n",
    "\n",
    "\n",
    "    def fit(self, X, y, loss=None):\n",
    "        \"\"\" Build decision tree \"\"\"\n",
    "        self.cat_features = X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "        self.con_features = [col for col in X.columns if col not in self.cat_features]\n",
    "        self.features_ = X.columns.tolist()\n",
    "        self.target_ = y.name\n",
    "        self.one_dim = len(np.shape(y)) == 1\n",
    "        # # Check if expansion of y is needed\n",
    "        # if len(np.shape(y)) == 1:\n",
    "        #     y = np.expand_dims(y, axis=1)\n",
    "\n",
    "        # Add y as last column of X\n",
    "        Xy = pd.concat((X, y), axis=1)\n",
    "        self.root = self._build_tree(Xy)\n",
    "        self.loss=None\n",
    "\n",
    "    def _build_tree(self, Xy, current_depth=0,features=None):\n",
    "        \"\"\" Recursive method which builds out the decision tree and splits X and respective y\n",
    "        on the feature of X which (based on impurity) best separates the data\"\"\"\n",
    "\n",
    "        largest_impurity = 0\n",
    "        bst_estimator = None    \n",
    "        bst_weights = None        \n",
    "        best_criteria = {\"feature_i\": None, \"threshold\": None} # Feature index and threshold\n",
    "        best_sets = {\n",
    "            \"leftXy\": None,   # Xy of left subtree\n",
    "            \"rightXy\": None # Xy of right subtree\n",
    "            }# Subsets of the data\n",
    "        \n",
    "        n_samples, n_features = np.shape(Xy)\n",
    "        if len(self.features_) + 1  == n_features:\n",
    "            if self.verbose : print(f\"\\nWe are going good at {current_depth} depth\")\n",
    "        else:\n",
    "            print('Something is wrong',n_samples, n_features, self.features_)\n",
    "\n",
    "        if n_samples >= self.min_samples_split and current_depth <= self.max_depth:\n",
    "            # Calculate the impurity for each feature\n",
    "            for feature_i in self.features_:\n",
    "                if self.verbose : print(f'Evaluating {feature_i}')\n",
    "                # All values of feature_i       \n",
    "                if feature_i in self.cat_features:\n",
    "                    unique_values = get_sorted_cats(Xy[feature_i].values)\n",
    "                    thresh = []\n",
    "                else:\n",
    "                    if self.digits is not None:\n",
    "                        unique_values = np.unique(np.round(Xy[feature_i],self.digits))\n",
    "                    else:\n",
    "                        unique_values = np.unique(Xy[feature_i])\n",
    "                    \n",
    "                # Iterate through all unique values of feature column i and\n",
    "                # calculate the impurity\n",
    "                for threshold in unique_values:\n",
    "                    # Divide X and y depending on if the feature value of X at index feature_i\n",
    "                    # meets the threshold\n",
    "                    if feature_i in self.cat_features:\n",
    "                        thresh.append(threshold)\n",
    "                        Xy1, Xy2 = divide_on_feature(Xy, feature_i, thresh)  \n",
    "                    else:\n",
    "                        Xy1, Xy2 = divide_on_feature(Xy, feature_i, threshold)\n",
    "\n",
    "                    if len(Xy1) > 0 and len(Xy2) > 0:\n",
    "                        \n",
    "                        # Select the y-values of the two sets\n",
    "                        # y1 = Xy1[:, n_features:]\n",
    "                        # y2 = Xy2[:, n_features:]\n",
    "                \n",
    "                        # Calculate impurity\n",
    "                        # impurity = self._impurity_calculation(y, y1, y2)\n",
    "                        \n",
    "                        # evaluate node function\n",
    "                        impurity, estimator, weights, addons = self._node_function(Xy, Xy1, Xy2, target=self.target_,features=self.features_)\n",
    "\n",
    "                        # If this threshold resulted in a higher information gain than previously\n",
    "                        # recorded save the threshold value and the feature\n",
    "                        # index\n",
    "                        if impurity > largest_impurity:\n",
    "                            largest_impurity, bst_estimator, bst_weights, bst_addons = impurity, estimator, weights, addons\n",
    "                            best_criteria[\"feature_i\"]= feature_i\n",
    "                            best_criteria[\"threshold\"] =  thresh if feature_i in self.cat_features else threshold\n",
    "                            best_sets[\"leftXy\"]=  Xy1 # X of left subtree\n",
    "                            best_sets[\"rightXy\"]=  Xy2 # X of left subtree\n",
    "                            \n",
    "                if self.verbose: print(f\"Impurity Update at {best_criteria} with {largest_impurity}\")\n",
    "                            \n",
    "                                \n",
    "        if largest_impurity > self.min_impurity:\n",
    "            if self.verbose: print(f'Best Impurity is with {best_criteria} with {largest_impurity} Split sizes: Left: {best_sets[\"leftXy\"].shape} : Right {best_sets[\"rightXy\"].shape}')\n",
    "            # Build subtrees for the right and left branches\n",
    "            true_branch = self._build_tree(best_sets[\"leftXy\"], current_depth + 1)\n",
    "            false_branch = self._build_tree(best_sets[\"rightXy\"], current_depth + 1)\n",
    "            node = DecisionNode(feature_i=best_criteria[\"feature_i\"], threshold=best_criteria[\n",
    "                                \"threshold\"], true_branch=true_branch, false_branch=false_branch,depth=current_depth)\n",
    "            node.estimator = bst_estimator\n",
    "            node.w = bst_weights\n",
    "            node.addons = bst_addons\n",
    "            return node\n",
    "        else:\n",
    "            if self.verbose: print(f\"No Impurity update. Hence a leaf with {best_criteria} with {largest_impurity}\")\n",
    "\n",
    "        # We're at leaf => determine value\n",
    "        leaf_value,estimator,weights,addons = self._leaf_function(Xy, target = self.target_, features=self.features_)\n",
    "        leaf_node = DecisionNode(value=leaf_value)\n",
    "        leaf_node.estimator = estimator\n",
    "        leaf_node.w = weights\n",
    "        leaf_node.addons = addons\n",
    "        return leaf_node\n",
    "\n",
    "\n",
    "    def predict_value(self, x, tree=None,verbose=False):\n",
    "        \"\"\" Do a recursive search down the tree and make a prediction of the data sample by the\n",
    "            value of the leaf that we end up at \"\"\"\n",
    "\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "\n",
    "        # If we have a value (i.e we're at a leaf) => return value as the prediction\n",
    "        if tree.value is not None:\n",
    "            return tree.estimate(x)\n",
    "\n",
    "        # Choose the feature that we will test\n",
    "        if isinstance(x, pd.Series):\n",
    "            feature_value = x.loc[tree.feature_i]\n",
    "        else:\n",
    "            feature_value = x[tree.feature_i].values[0]\n",
    "        if verbose:\n",
    "            print(f'Feature value for {tree.feature_i} :{feature_value}')\n",
    "\n",
    "        # Determine if we will follow left or right branch\n",
    "        branch = tree.false_branch\n",
    "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "            if feature_value >= tree.threshold:  branch = tree.true_branch\n",
    "        elif isinstance(tree.threshold,list):\n",
    "            if feature_value in tree.threshold: branch = tree.true_branch\n",
    "        elif feature_value == tree.threshold: \n",
    "            branch = tree.true_branch\n",
    "\n",
    "        # Test subtree\n",
    "        return self.predict_value(x, branch,verbose)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Classify samples one by one and return the set of labels \"\"\"\n",
    "        y_pred = [self.predict_value(sample) for sample in X]\n",
    "        return y_pred\n",
    "\n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        \"\"\" Recursively print the decision tree \"\"\"\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        # If we're at leaf => print the label\n",
    "        if tree.value is not None:\n",
    "            print (tree.value)\n",
    "        # Go deeper down the tree\n",
    "        else:\n",
    "            # Print test\n",
    "            print (\"%s:%s? \" % (tree.feature_i, tree.threshold))\n",
    "            # Print the true scenario\n",
    "            print (\"%sT->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.true_branch, indent + indent)\n",
    "            # Print the false scenario\n",
    "            print (\"%sF->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.false_branch, indent + indent)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
