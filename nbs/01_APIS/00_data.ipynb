{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "> A simple adaptor class to make the data in required format that can be easily consumed and processd by the models.\n",
    "> The key aspect here is to make adaptable and fast prcessable dataset, reduce the data size, make splits and add any processing if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jitink/applications/miniconda3/envs/mltreedev/lib/python3.8/site-packages/nbdev/export.py:54: UserWarning: Notebook '/home/jitink/Documents/Explorations/mltreelib/nbs/00_Examples/00_dataset.ipynb' uses `#|export` without `#|default_exp` cell.\n",
      "Note nbdev2 no longer supports nbdev1 syntax. Run `nbdev_migrate` to upgrade.\n",
      "See https://nbdev.fast.ai/getting_started.html for more information.\n",
      "  warn(f\"Notebook '{nbname}' uses `#|export` without `#|default_exp` cell.\\n\"\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.utils import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Dataset Adaptor Class\n",
    "\n",
    "    This class is meant to make dataset possible which would be consumed by models further\n",
    "    \"\"\"\n",
    "    def __init__(self, df:pd.DataFrame,      # Dataframe that needs to be converted \n",
    "                reduce_datatype:bool = True, # Shall we try to reduce datatype to make is smaller\n",
    "                encode_category:str = None,  # Do encoding of categories default to None as no encoding\n",
    "                add_intercept:bool = False,  # Add a constant value intercept to data. This might be needed for Model based Trees.\n",
    "                na_treatment:str = 'allow',  # How to work with nas. Default: 'allow'\n",
    "                copy_data:bool = False,      # Keep a self copy of original data\n",
    "                digits:int = None,           # To round float to certain digits or not, Default: None means no rounding\n",
    "                n_category:Union[int, float, None] = None  # How many different level shoud be treated as category. If a value less than one the number of levels is defined aas % oft total rows\n",
    "                ) -> None:\n",
    "        self.df, self.reduce_datatype = df, reduce_datatype\n",
    "        self.encode_category, self.add_intercept = encode_category, add_intercept\n",
    "        self.na_treatment, self.copy,self.digits = na_treatment, copy_data, digits\n",
    "        self.n_category = n_category\n",
    "        self.split_ratio = None\n",
    "        self.features_ = self.df.columns.tolist()\n",
    "        self._shape = self.df.shape\n",
    "        self._dtypes = self.df.dtypes\n",
    "        self._category_lbl_dict = {}\n",
    "        self._dataranges = {}\n",
    "        \n",
    "        self._process_data()      \n",
    "\n",
    "    def _reduce_size(self):\n",
    "        df = pd.DataFrame()\n",
    "        for col,dtype in self._dtypes.items():\n",
    "            if dtype != object:\n",
    "                max_,min_,hasna_ = self.df[col].max(),self.df[col].min(),np.isfinite(self.df[col]).all()\n",
    "                isint_ = dtype == int\n",
    "                if not hasna_:\n",
    "                    asint = self.df[col].fillna(0).astype(np.int64)\n",
    "                    result = (self.df[col] - asint).sum()\n",
    "                    if -0.01 < result < 0.01:\n",
    "                        isint_ = True\n",
    "\n",
    "                # Make Integer/unsigned Integer datatypes\n",
    "                if isint_:\n",
    "                    if min_ >= 0:\n",
    "                        df[col] = pd.to_numeric(self.df[col].fillna(min_-1), downcast=\"unsigned\",errors='coerce')\n",
    "                    else:\n",
    "                        df[col] = pd.to_numeric(self.df[col].fillna(min_-1), downcast=\"integer\",errors='coerce')\n",
    "                        \n",
    "                    self._dataranges[col] = {'hasna':hasna_,'min':min_-1,'max':max_,'inferredtype':df[col].dtype}\n",
    "\n",
    "                # Make float datatypes 32 bit\n",
    "                else: #todo make this more advanced with rounding and more : evaluate the below an\n",
    "                    if self.digits is None:\n",
    "                        df[col] = pd.to_numeric(self.df[col], downcast=\"float\",errors='coerce')\n",
    "                    else:\n",
    "                        df[col] = pd.to_numeric(np.round(self.df[col],self.digits), downcast=\"float\",errors='coerce')\n",
    "                        max_,min_,hasna_ = df[col].max(),df[col].min(),np.isfinite(df[col]).all()\n",
    "                    self._dataranges[col] = {'hasna':hasna_,'min':min_,'max':max_,'inferredtype':df[col].dtype}\n",
    "            else:\n",
    "                #todo : add logic of makig a bing object to small category based on defintion\n",
    "                df[col] =self.df[col]\n",
    "                self._dataranges[col] = {'hasna':hasna_,'min':None,'max':None,'inferredtype':df[col].dtype}\n",
    "        \n",
    "        try: \n",
    "            self.data = np.array([tuple(x) for x in df.values], dtype=[(k,v['inferredtype']) for k,v in self._dataranges.items()])\n",
    "        except:\n",
    "            print('Their was an error in above')\n",
    "            self.data = df.copy(deep=True)\n",
    "\n",
    "    def _process_data(self):\n",
    "        self._reduce_size()\n",
    "        if not self.copy:\n",
    "            self.df = None\n",
    "            gc.collect()\n",
    "\n",
    "    def dtypes(self,actual=False):\n",
    "        if actual:\n",
    "            return pd.DataFrame.from_dict(self._dtypes,orient='index').rename(columns={0:'data_type'})\n",
    "        else:\n",
    "            dt = {k:v['inferredtype'] for k,v in self._dataranges.items()}\n",
    "\n",
    "    @property\n",
    "    def shape(self): return self._shape\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        s = f\"Dataset(df=Shape({self.shape}\"\n",
    "        for k,v in self.__dict__.items() :\n",
    "            if (k not in ('df','features_','data')) and (not k.startswith('_')) : s += f\", {k}={v}\"\n",
    "        s += ')'\n",
    "        return s\n",
    "    \n",
    "    __repr__ = __str__\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to work on data\n",
    "\n",
    "\n",
    "Please refer `Examples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# data.features_\n",
    "# dummy_data.to_dict?\n",
    "# data._dtypes = data.df.dtypes.to_dict()\n",
    "# data._dtypes\n",
    "# dummy_data.rename?\n",
    "# {k:v==object for k,v in data._dtypes.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mltreedev')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
