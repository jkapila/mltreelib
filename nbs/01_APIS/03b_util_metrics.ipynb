{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions\n",
    "\n",
    "> More on this soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.utils import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import numba\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    \"\"\" Returns the mean squared error between y_true and y_pred \"\"\"\n",
    "    mse = np.mean(np.power(y_true - y_pred, 2))\n",
    "    return mse\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    \"\"\" Returns the mean squared error between y_true and y_pred \"\"\"\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    return mae\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\" Compare y_true to y_pred and return the accuracy \"\"\"\n",
    "    accuracy = np.sum(y_true == y_pred, axis=0) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# To implement\n",
    "# metric(s) to be evaluated on the evaluation set(s)\n",
    "# \"\" (empty string or not specified) means that metric corresponding to specified objective will be used (this is possible only for pre-defined objective functions, otherwise no evaluation metric will be added)\n",
    "# \"None\" (string, not a None value) means that no metric will be registered, aliases: na, null, custom\n",
    "# l1, absolute loss, aliases: mean_absolute_error, mae, regression_l1\n",
    "# l2, square loss, aliases: mean_squared_error, mse, regression_l2, regression\n",
    "# nmse normalized mean square error\n",
    "# mdape, median variant of these\n",
    "# rmse, root square loss, aliases: root_mean_squared_error, l2_root\n",
    "# quantile, Quantile regression\n",
    "# mape, MAPE loss, aliases: mean_absolute_percentage_error\n",
    "# huber, Huber loss\n",
    "# fair, Fair loss\n",
    "# poisson, negative log-likelihood for Poisson regression\n",
    "# gamma, negative log-likelihood for Gamma regression\n",
    "# gamma_deviance, residual deviance for Gamma regression\n",
    "# tweedie, negative log-likelihood for Tweedie regression\n",
    "# map, MAP, aliases: mean_average_precision\n",
    "# auc, AUC\n",
    "# average_precision, average precision score\n",
    "# binary_logloss, log loss, aliases: binary\n",
    "# binary_error, for one sample: 0 for correct classification, 1 for error classification\n",
    "# auc_mu, AUC-mu\n",
    "# multi_logloss, log loss for multi-class classification, aliases: multiclass, softmax, multiclassova, multiclass_ova, ova, ovr\n",
    "# multi_error, error rate for multi-class classification\n",
    "# cross_entropy, cross-entropy (with optional linear weights), aliases: xentropy\n",
    "# cross_entropy_lambda, “intensity-weighted” cross-entropy, aliases: xentlambda\n",
    "# kullback_leibler, Kullback-Leibler divergence, aliases: kldiv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
