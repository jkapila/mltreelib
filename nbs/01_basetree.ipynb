{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp basetree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaseTree\n",
    "\n",
    "> More on this soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.utils import *\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def divide_on_feature(X, feature_i, threshold):\n",
    "    \"\"\" Divide dataset based on if sample value on feature index is larger than\n",
    "        the given threshold \"\"\"\n",
    "    split_func = None\n",
    "    if isinstance(threshold, int) or isinstance(threshold, float):\n",
    "        split_func = lambda sample: sample[feature_i] >= threshold\n",
    "    elif isinstance(threshold,list):\n",
    "        split_func = lambda sample: sample[feature_i].isin(threshold)\n",
    "    else:\n",
    "        split_func = lambda sample: sample[feature_i] == threshold\n",
    "\n",
    "    # X_1 = np.array([sample for sample in X if split_func(sample)])\n",
    "    # X_2 = np.array([sample for sample in X if not split_func(sample)])\n",
    "    # return np.array([X_1, X_2])\n",
    "\n",
    "    mask = split_func(X)\n",
    "    X_1 = X[mask]\n",
    "    X_2 = X[~mask]\n",
    "    return X_1,X_2\n",
    "    \n",
    "def get_sorted_cats(x,ascending=True):\n",
    "    u, count = np.unique(x, return_counts=True)\n",
    "    if ascending:\n",
    "        count_sort_ind = np.argsort(count)\n",
    "    else:\n",
    "        count_sort_ind = np.argsort(-count)\n",
    "    return u[count_sort_ind]\n",
    "\n",
    "class DecisionNode():\n",
    "    \"\"\"Class that represents a decision node or leaf in the decision tree\n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_i: int\n",
    "        Feature index which we want to use as the threshold measure.\n",
    "    threshold: float\n",
    "        The value that we will compare feature values at feature_i against to\n",
    "        determine the prediction.\n",
    "    value: float\n",
    "        The class prediction if classification tree, or float value if regression tree.\n",
    "    true_branch: DecisionNode\n",
    "        Next decision node for samples where features value met the threshold.\n",
    "    false_branch: DecisionNode\n",
    "        Next decision node for samples where features value did not meet the threshold.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_i=None, threshold=None, \n",
    "                 value=None, true_branch=None, false_branch=None, depth=None):\n",
    "        self.feature_i = feature_i          # Index for the feature that is tested\n",
    "        self.threshold = threshold          # Threshold value for feature\n",
    "        self.value = value                  # Value if the node is a leaf in the tree\n",
    "        self.true_branch = true_branch      # 'Left' subtree\n",
    "        self.false_branch = false_branch    # 'Right' subtree\n",
    "        self.estimator =  None\n",
    "        self.is_leaf = False \n",
    "        self.loss = None\n",
    "        self.grad = None\n",
    "        self.w = None\n",
    "        self.resid = None\n",
    "        self.depth = depth\n",
    "        self.addons = None\n",
    "        \n",
    "    def estimate(self,x):\n",
    "        return self.value if self.estimator is None else self.estimator.predict(x)\n",
    "\n",
    "# Super class of RegressionTree and ClassificationTree\n",
    "class BaseDecisionTree(object):\n",
    "    \"\"\"Super class of RegressionTree and ClassificationTree.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    min_samples_split: int\n",
    "        The minimum number of samples needed to make a split when building a tree.\n",
    "    min_impurity: float\n",
    "        The minimum impurity required to split the tree further.\n",
    "    max_depth: int\n",
    "        The maximum depth of a tree.\n",
    "    loss: function\n",
    "        Loss function that is used for Gradient Boosting models to calculate impurity.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_samples_split=2, min_impurity=1e-7,\n",
    "                 max_depth=float(\"inf\"), loss=None,verbose=False, round_float=True,round_digit=4):\n",
    "        self.root = None  # Root node in dec. tree\n",
    "        # Minimum n of samples to justify split\n",
    "        self.min_samples_split = min_samples_split\n",
    "        # The minimum impurity to justify split\n",
    "        self.min_impurity = min_impurity\n",
    "        # The maximum depth to grow the tree to\n",
    "        self.max_depth = max_depth\n",
    "        # Function to calculate impurity (classif.=>info gain, regr=>variance reduct.)\n",
    "        self._node_function = None\n",
    "        # Function to determine prediction of y at leaf\n",
    "        self._leaf_function = None\n",
    "        # If y is one-hot encoded (multi-dim) or not (one-dim)\n",
    "        self.one_dim = None\n",
    "        # If Gradient Boost\n",
    "        self.loss = loss\n",
    "        self.cat_features = None\n",
    "        self.con_features = None\n",
    "        self.features_ = None\n",
    "        self.target_ = None\n",
    "        self.round_float = round_float\n",
    "        self.round_digit = round_digit\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, X, y, loss=None):\n",
    "        \"\"\" Build decision tree \"\"\"\n",
    "        self.cat_features = X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "        self.con_features = [col for col in X.columns if col not in self.cat_features]\n",
    "        self.features_ = X.columns.tolist()\n",
    "        self.target_ = y.name\n",
    "        self.one_dim = len(np.shape(y)) == 1\n",
    "        # # Check if expansion of y is needed\n",
    "        # if len(np.shape(y)) == 1:\n",
    "        #     y = np.expand_dims(y, axis=1)\n",
    "\n",
    "        # Add y as last column of X\n",
    "        Xy = pd.concat((X, y), axis=1)\n",
    "        self.root = self._build_tree(Xy)\n",
    "        self.loss=None\n",
    "\n",
    "    def _build_tree(self, Xy, current_depth=0,features=None):\n",
    "        \"\"\" Recursive method which builds out the decision tree and splits X and respective y\n",
    "        on the feature of X which (based on impurity) best separates the data\"\"\"\n",
    "\n",
    "        largest_impurity = 0\n",
    "        bst_estimator = None    \n",
    "        bst_weights = None        \n",
    "        best_criteria = {\"feature_i\": None, \"threshold\": None} # Feature index and threshold\n",
    "        best_sets = {\n",
    "            \"leftXy\": None,   # Xy of left subtree\n",
    "            \"rightXy\": None # Xy of right subtree\n",
    "            }# Subsets of the data\n",
    "        \n",
    "        n_samples, n_features = np.shape(Xy)\n",
    "        if len(self.features_) + 1  == n_features:\n",
    "            if self.verbose : print(f\"\\nWe are going good at {current_depth} depth\")\n",
    "        else:\n",
    "            print('Somethign is wrong',n_samples, n_features, self.features_)\n",
    "\n",
    "        if n_samples >= self.min_samples_split and current_depth <= self.max_depth:\n",
    "            # Calculate the impurity for each feature\n",
    "            for feature_i in self.features_:\n",
    "                if self.verbose : print(f'Evaluating {feature_i}')\n",
    "                # All values of feature_i       \n",
    "                if feature_i in self.cat_features:\n",
    "                    unique_values = get_sorted_cats(Xy[feature_i].values)\n",
    "                    thresh = []\n",
    "                else:\n",
    "                    if self.round_float:\n",
    "                        unique_values = np.unique(np.round(Xy[feature_i],self.round_digit))\n",
    "                    else:\n",
    "                        unique_values = np.unique(Xy[feature_i])\n",
    "                    \n",
    "                # Iterate through all unique values of feature column i and\n",
    "                # calculate the impurity\n",
    "                for threshold in unique_values:\n",
    "                    # Divide X and y depending on if the feature value of X at index feature_i\n",
    "                    # meets the threshold\n",
    "                    if feature_i in self.cat_features:\n",
    "                        thresh.append(threshold)\n",
    "                        Xy1, Xy2 = divide_on_feature(Xy, feature_i, thresh)  \n",
    "                    else:\n",
    "                        Xy1, Xy2 = divide_on_feature(Xy, feature_i, threshold)\n",
    "\n",
    "                    if len(Xy1) > 0 and len(Xy2) > 0:\n",
    "                        \n",
    "                        # Select the y-values of the two sets\n",
    "                        # y1 = Xy1[:, n_features:]\n",
    "                        # y2 = Xy2[:, n_features:]\n",
    "                \n",
    "                        # Calculate impurity\n",
    "                        # impurity = self._impurity_calculation(y, y1, y2)\n",
    "                        \n",
    "                        # evaluate node function\n",
    "                        impurity, estimator, weights, addons = self._node_function(Xy, Xy1,Xy2,target,features=features)\n",
    "\n",
    "                        # If this threshold resulted in a higher information gain than previously\n",
    "                        # recorded save the threshold value and the feature\n",
    "                        # index\n",
    "                        if impurity > largest_impurity:\n",
    "                            largest_impurity, bst_estimator, bst_weights, bst_addons = impurity, estimator, weights, addons\n",
    "                            best_criteria[\"feature_i\"]= feature_i\n",
    "                            best_criteria[\"threshold\"] =  thresh if feature_i in self.cat_features else threshold\n",
    "                            best_sets[\"leftXy\"]=  Xy1 # X of left subtree\n",
    "                            best_sets[\"rightXy\"]=  Xy2 # X of left subtree\n",
    "                            \n",
    "                if self.verbose: print(f\"Impurity Update at {best_criteria} with {largest_impurity}\")\n",
    "                            \n",
    "                                \n",
    "        if largest_impurity > self.min_impurity:\n",
    "            if self.verbose: print(f'Best Impurtiy is with {best_criteria} with {largest_impurity} Split sizes: Left: {best_sets[\"leftXy\"].shape} : Right {best_sets[\"rightXy\"].shape}')\n",
    "            # Build subtrees for the right and left branches\n",
    "            true_branch = self._build_tree(best_sets[\"leftXy\"], current_depth + 1)\n",
    "            false_branch = self._build_tree(best_sets[\"rightXy\"], current_depth + 1)\n",
    "            node = DecisionNode(feature_i=best_criteria[\"feature_i\"], threshold=best_criteria[\n",
    "                                \"threshold\"], true_branch=true_branch, false_branch=false_branch,depth=current_depth)\n",
    "            node.estimator = bst_estimator\n",
    "            node.w = bst_weights\n",
    "            node.addons = bst_addons\n",
    "            return node\n",
    "        else:\n",
    "            if self.verbose: print(f\"No Impurtiy update. Hence a leaf with {best_criteria} with {largest_impurity}\")\n",
    "\n",
    "        # We're at leaf => determine value\n",
    "        leaf_value,estimator,weights,addons = self._leaf_function(Xy, target, features=features)\n",
    "        leaf_node = DecisionNode(value=leaf_value)\n",
    "        leaf_node.estimator = estimator\n",
    "        leaf_node.w = weights\n",
    "        leaf_node.addons = addons\n",
    "        return leaf_node\n",
    "\n",
    "\n",
    "    def predict_value(self, x, tree=None,verbose=False):\n",
    "        \"\"\" Do a recursive search down the tree and make a prediction of the data sample by the\n",
    "            value of the leaf that we end up at \"\"\"\n",
    "\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "\n",
    "        # If we have a value (i.e we're at a leaf) => return value as the prediction\n",
    "        if tree.value is not None:\n",
    "            return tree.estimate(x)\n",
    "\n",
    "        # Choose the feature that we will test\n",
    "        if isinstance(x, pd.Series):\n",
    "            feature_value = x.loc[tree.feature_i]\n",
    "        else:\n",
    "            feature_value = x[tree.feature_i].values[0]\n",
    "        if verbose:\n",
    "            print(f'Feature value for {tree.feature_i} :{feature_value}')\n",
    "\n",
    "        # Determine if we will follow left or right branch\n",
    "        branch = tree.false_branch\n",
    "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "            if feature_value >= tree.threshold:  branch = tree.true_branch\n",
    "        elif isinstance(tree.threshold,list):\n",
    "            if feature_value in tree.threshold: branch = tree.true_branch\n",
    "        elif feature_value == tree.threshold: \n",
    "            branch = tree.true_branch\n",
    "\n",
    "        # Test subtree\n",
    "        return self.predict_value(x, branch,verbose)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Classify samples one by one and return the set of labels \"\"\"\n",
    "        y_pred = [self.predict_value(sample) for sample in X]\n",
    "        return y_pred\n",
    "\n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        \"\"\" Recursively print the decision tree \"\"\"\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        # If we're at leaf => print the label\n",
    "        if tree.value is not None:\n",
    "            print (tree.value)\n",
    "        # Go deeper down the tree\n",
    "        else:\n",
    "            # Print test\n",
    "            print (\"%s:%s? \" % (tree.feature_i, tree.threshold))\n",
    "            # Print the true scenario\n",
    "            print (\"%sT->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.true_branch, indent + indent)\n",
    "            # Print the false scenario\n",
    "            print (\"%sF->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.false_branch, indent + indent)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mltreedev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aa459615d486078b116a72c354f12cd7cc759d106bcc5e574dd99ad999f0993f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
