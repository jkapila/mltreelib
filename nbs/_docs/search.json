[
  {
    "objectID": "00_Examples/core.html",
    "href": "00_Examples/core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  },
  {
    "objectID": "00_Examples/00_dataset.html",
    "href": "00_Examples/00_dataset.html",
    "title": "How to work with Datasets",
    "section": "",
    "text": "n_size = 1000000\nrnd = np.random.RandomState(1234)\ndummy_data = pd.DataFrame({'numericfull':rnd.randint(1,500,size=n_size),\n                            'unitint':rnd.randint(1,25,size=n_size),\n                            'floatfull':rnd.random_sample(size=n_size),\n                            'floatsmall':np.round(rnd.random_sample(size=n_size)+rnd.randint(1,25,size=n_size),2),\n                            'categoryobj':rnd.choice(['a','b','c','d'],size=n_size),\n                            'stringobj':rnd.choice([\"{:c}\".format(k) for k in range(97, 123)],size=n_size)})\ndummy_data.head()\n\n\n\n\n\n  \n    \n      \n      numericfull\n      unitint\n      floatfull\n      floatsmall\n      categoryobj\n      stringobj\n    \n  \n  \n    \n      0\n      304\n      1\n      0.651859\n      11.42\n      a\n      f\n    \n    \n      1\n      212\n      1\n      0.906869\n      23.28\n      d\n      v\n    \n    \n      2\n      295\n      23\n      0.933262\n      21.79\n      d\n      t\n    \n    \n      3\n      54\n      19\n      0.919103\n      9.24\n      d\n      s\n    \n    \n      4\n      205\n      9\n      0.262066\n      16.69\n      a\n      l\n    \n  \n\n\n\n\nPass it to Dataset and let it do its magic\n\ndataset = Dataset(df=dummy_data)\ndataset\n\nDataset(df=Shape((1000000, 6), reduce_datatype=True, encode_category=None, add_intercept=False, na_treatment=allow, copy=False, digits=None, n_category=None, split_ratio=None)\n\n\nTo acess raw processed data\n\ndataset.data[:5]\n\narray([(304,  1, 0.65185905, 11.42, 'a', 'f'),\n       (212,  1, 0.90686905, 23.28, 'd', 'v'),\n       (295, 23, 0.9332624 , 21.79, 'd', 't'),\n       ( 54, 19, 0.9191031 ,  9.24, 'd', 's'),\n       (205,  9, 0.2620663 , 16.69, 'a', 'l')],\n      dtype=[('numericfull', '<u2'), ('unitint', 'u1'), ('floatfull', '<f4'), ('floatsmall', '<f4'), ('categoryobj', 'O'), ('stringobj', 'O')])\n\n\nNote: This is a Structured arrays and not a simmple numpy array or pandas data frame.\nSize reduction is as follows:\n\nprint('Pandas Data Frame        : ',np.round(dummy_data.memory_usage(deep=True).sum()*1e-6,2),'MB')\nprint('Dataset Structured Array : ',np.round(dataset.data.nbytes*1e-6/ 1024 * 1024,2),'MB')\n\nPandas Data Frame        :  148.0 MB\nDataset Structured Array :  27.0 MB\n\n\n\nprint(dummy_data.info(memory_usage='deep'))\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 6 columns):\n #   Column       Non-Null Count    Dtype  \n---  ------       --------------    -----  \n 0   numericfull  1000000 non-null  int64  \n 1   unitint      1000000 non-null  int64  \n 2   floatfull    1000000 non-null  float64\n 3   floatsmall   1000000 non-null  float64\n 4   categoryobj  1000000 non-null  object \n 5   stringobj    1000000 non-null  object \ndtypes: float64(2), int64(2), object(2)\nmemory usage: 141.1 MB\nNone\n\n\n\nFurther reduction in data size\nWe can even further reduce data by using following parameters:\n\ndataset = Dataset(df=dummy_data, digits=2)\nprint('Pandas Data Frame        : ',np.round(dummy_data.memory_usage(deep=True).sum()*1e-6,2),'MB')\nprint('Dataset Structured Array : ',np.round(dataset.data.nbytes*1e-6/ 1024 * 1024,2),'MB')\n\nPandas Data Frame        :  148.0 MB\nDataset Structured Array :  27.0 MB\n\n\n\ndataset.data[:5]\n\narray([(304,  1, 0.65, 11.42, 'a', 'f'), (212,  1, 0.91, 23.28, 'd', 'v'),\n       (295, 23, 0.93, 21.79, 'd', 't'), ( 54, 19, 0.92,  9.24, 'd', 's'),\n       (205,  9, 0.26, 16.69, 'a', 'l')],\n      dtype=[('numericfull', '<u2'), ('unitint', 'u1'), ('floatfull', '<f4'), ('floatsmall', '<f4'), ('categoryobj', 'O'), ('stringobj', 'O')])"
  },
  {
    "objectID": "01_APIS/base.tree.html",
    "href": "01_APIS/base.tree.html",
    "title": "BaseTree",
    "section": "",
    "text": "source\n\nBaseDecisionTree\n\n BaseDecisionTree (objective:str='class', min_samples_split:int=20,\n                   min_sample_leaf:int=10, max_compete:int=4,\n                   min_impurity:float=0.01, max_depth:int=inf,\n                   max_surrogates:int=2, surrogate_style:int=2,\n                   parallelize:str='feature', tree_growth:str='cart',\n                   feature_weights:Union[float,int,list]=None,\n                   loss:Union[str,float]=None,\n                   verbose:Union[bool,int]=False,\n                   digits:Union[int,NoneType]=2)\n\nSuper class of all kinds of tree.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nobjective\nstr\nclass\ndefining objective of the whole tree building.\n\n\nmin_samples_split\nint\n20\nThe minimum number of samples needed to make a split when building a tree.\n\n\nmin_sample_leaf\nint\n10\nMinimum sample required to have a leaf node\n\n\nmax_compete\nint\n4\nthe number of competitor splits retained in the output. It is useful to know not just which split was chosen, but which variable came in second, third, etc.\n\n\nmin_impurity\nfloat\n0.01\nThe minimum impurity required to split the tree further. this is equivalent to complexity in CART\n\n\nmax_depth\nint\ninf\nThe maximum depth of a tree.\n\n\nmax_surrogates\nint\n2\nmore on this to make it generalized and not CART Specific\n\n\nsurrogate_style\nint\n2\nmore on this to make it generalized and not CART Specific\n\n\nparallelize\nstr\nfeature\nfollow form LightGBMs behavior\n\n\ntree_growth\nstr\ncart\nfollow binary structure if cart else follow multiple structure as C4.5\n\n\nfeature_weights\ntyping.Union[float, int, list]\nNone\nweight of each feature in the split. Default is set to 1 for all features\n\n\nloss\ntyping.Union[str, float]\nNone\nString of loss or function which defines the loss. This amounts to Loss function that is used for Gradient Boosting models to calculate impurity.\n\n\nverbose\ntyping.Union[bool, int]\nFalse\nVerbosity for tree building\n\n\ndigits\ntyping.Union[int, NoneType]\n2\nTo round the values before doing a split\n\n\n\n\nsource\n\n\nDecisionNode\n\n DecisionNode (feature_i:Union[str,int]=None, threshold:float=None,\n               value=None, true_branch=None, false_branch=None)\n\nClass that represents a decision node or leaf in the decision tree\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfeature_i\ntyping.Union[str, int]\nNone\nFeature index which we want to use as the threshold measure.\n\n\nthreshold\nfloat\nNone\nThe value that we will compare feature values at feature_i against to determine the prediction.\n\n\nvalue\nNoneType\nNone\nThe class prediction if classification tree, or float value if regression tree.\n\n\ntrue_branch\nNoneType\nNone\nNext decision node for samples where features value met the threshold.\n\n\nfalse_branch\nNoneType\nNone\nNext decision node for samples where features value did not meet the threshold.\n\n\n\n\nsource\n\n\nNode\n\n Node (nodeid:int=None, depth:int=None, split:Union[str,int,list]=None,\n       kids:Union[NoneType,list]=None,\n       surrogates:Union[NoneType,list]=None, n_subnodes:int=2,\n       info:str=None)\n\nA generics node for all tress\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnodeid\nint\nNone\nAs simple id of the node\n\n\ndepth\nint\nNone\ndepth of current node\n\n\nsplit\ntyping.Union[str, int, list]\nNone\nSplit definitions, New implementation coming\n\n\nkids\ntyping.Union[NoneType, list]\nNone\nNew implementation coming\n\n\nsurrogates\ntyping.Union[NoneType, list]\nNone\nNew implementation coming\n\n\nn_subnodes\nint\n2\nDefault for CART behaviors, if None then would assume C4.5 / ID3 behavior\n\n\ninfo\nstr\nNone\njust a generic info about the node\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nCART\n\n CART (objective:str='class', min_samples_split:int=20,\n       min_sample_leaf:int=10, max_compete:int=4, min_impurity:float=0.01,\n       max_depth:int=inf, max_surrogates:int=2, surrogate_style:int=2,\n       parallelize:str='feature', tree_growth:str='cart',\n       feature_weights:Union[float,int,list]=None,\n       loss:Union[str,float]=None, verbose:Union[bool,int]=False,\n       digits:Union[int,NoneType]=2)\n\nSuper class of all kinds of tree.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nobjective\nstr\nclass\ndefining objective of the whole tree building.\n\n\nmin_samples_split\nint\n20\nThe minimum number of samples needed to make a split when building a tree.\n\n\nmin_sample_leaf\nint\n10\nMinimum sample required to have a leaf node\n\n\nmax_compete\nint\n4\nthe number of competitor splits retained in the output. It is useful to know not just which split was chosen, but which variable came in second, third, etc.\n\n\nmin_impurity\nfloat\n0.01\nThe minimum impurity required to split the tree further. this is equivalent to complexity in CART\n\n\nmax_depth\nint\ninf\nThe maximum depth of a tree.\n\n\nmax_surrogates\nint\n2\nmore on this to make it generalized and not CART Specific\n\n\nsurrogate_style\nint\n2\nmore on this to make it generalized and not CART Specific\n\n\nparallelize\nstr\nfeature\nfollow form LightGBMs behavior\n\n\ntree_growth\nstr\ncart\nfollow binary structure if cart else follow multiple structure as C4.5\n\n\nfeature_weights\ntyping.Union[float, int, list]\nNone\nweight of each feature in the split. Default is set to 1 for all features\n\n\nloss\ntyping.Union[str, float]\nNone\nString of loss or function which defines the loss. This amounts to Loss function that is used for Gradient Boosting models to calculate impurity.\n\n\nverbose\ntyping.Union[bool, int]\nFalse\nVerbosity for tree building\n\n\ndigits\ntyping.Union[int, NoneType]\n2\nTo round the values before doing a split\n\n\n\n\nclass C45ClassificationTree(BaseDecisionTree):\n    def _calculate_information_gain(self, y, y1, y2):\n        # Calculate information gain\n        p = len(y1) / len(y)\n        entropy = calculate_entropy(y)\n        info_gain = entropy - p * \\\n            calculate_entropy(y1) - (1 - p) * \\\n            calculate_entropy(y2)\n\n        return info_gain\n\n    def _majority_vote(self, y):\n        most_common = None\n        max_count = 0\n        for label in np.unique(y):\n            # Count number of occurences of samples with label\n            count = len(y[y == label])\n            if count > max_count:\n                most_common = label\n                max_count = count\n        return most_common\n\n    def fit(self, X, y):\n        self._impurity_calculation = self._calculate_information_gain\n        self._leaf_value_calculation = self._majority_vote\n        super(C45ClassificationTree, self).fit(X, y)"
  },
  {
    "objectID": "01_APIS/utils.metrics.html",
    "href": "01_APIS/utils.metrics.html",
    "title": "Utility functions",
    "section": "",
    "text": "source\n\naccuracy_score\n\n accuracy_score (y_true, y_pred)\n\nCompare y_true to y_pred and return the accuracy\n\nsource\n\n\nmae\n\n mae (y_true, y_pred)\n\nReturns the mean squared error between y_true and y_pred\n\nsource\n\n\nmse\n\n mse (y_true, y_pred)\n\nReturns the mean squared error between y_true and y_pred"
  },
  {
    "objectID": "01_APIS/utils.extras.html",
    "href": "01_APIS/utils.extras.html",
    "title": "Utility functions",
    "section": "",
    "text": "source\n\nrbf_kernel\n\n rbf_kernel (gamma, **kwargs)\n\n\nsource\n\n\npolynomial_kernel\n\n polynomial_kernel (power, coef, **kwargs)\n\n\nsource\n\n\nlinear_kernel\n\n linear_kernel (**kwargs)"
  },
  {
    "objectID": "01_APIS/utils.functions.html",
    "href": "01_APIS/utils.functions.html",
    "title": "Utility functions",
    "section": "",
    "text": "source\n\ncalculate_correlation_matrix\n\n calculate_correlation_matrix (X, Y=None)\n\nCalculate the correlation matrix for the dataset X\n\nsource\n\n\ncalculate_covariance_matrix\n\n calculate_covariance_matrix (X, Y=None)\n\nCalculate the covariance matrix for the dataset X\n\nsource\n\n\neuclidean_distance\n\n euclidean_distance (x1, x2)\n\nCalculates the l2 distance between two vectors\n\nsource\n\n\ncalculate_std_dev\n\n calculate_std_dev (X)\n\nCalculate the standard deviations of the features in dataset X\n\nsource\n\n\ncalculate_variance\n\n calculate_variance (X)\n\nReturn the variance of the features in dataset X\n\nsource\n\n\ncalculate_rss\n\n calculate_rss (y)\n\nReturns the sum of residual squared error between y_true and y_mean\n\nsource\n\n\ncalculate_mae\n\n calculate_mae (y)\n\nReturns the mean absolute error between y_true and y_mean\n\nsource\n\n\ncalculate_mse\n\n calculate_mse (y)\n\nReturns the mean squared error between y_true and y_mean\n\nsource\n\n\nget_sorted_cats\n\n get_sorted_cats (x, ascending=True)\n\nGet sorted list of categorical levels in defined order\n\nsource\n\n\ndivide_on_feature\n\n divide_on_feature (X, feature_i, threshold)\n\nDivide dataset based on if sample value on feature index is larger than the given threshold\n\nsource\n\n\ncalculate_gini\n\n calculate_gini (y)\n\nGini impurity (local entropy) of a label sequence\n\nsource\n\n\ncalculate_entropy\n\n calculate_entropy (y)\n\nEntropy of a label sequence"
  },
  {
    "objectID": "01_APIS/tree.html",
    "href": "01_APIS/tree.html",
    "title": "Tree Implementations",
    "section": "",
    "text": "source\n\nTree\n\n Tree ()\n\nTree class enable building all simpe single tree models\n\nsource\n\n\nCARTRegressionTree\n\n CARTRegressionTree (min_samples_split:int=20, min_impurity:float=1e-07,\n                     min_sample_leaf:int=10, max_depth:int=inf,\n                     loss:str=None, verbose:Union[bool,int]=False,\n                     digits:Union[int,NoneType]=2)\n\nSuper class of RegressionTree and ClassificationTree.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmin_samples_split\nint\n20\nThe minimum number of samples needed to make a split when building a tree.\n\n\nmin_impurity\nfloat\n1e-07\nThe minimum impurity required to split the tree further.\n\n\nmin_sample_leaf\nint\n10\nMinimum sample required to have a leaf node\n\n\nmax_depth\nint\ninf\nThe maximum depth of a tree.\n\n\nloss\nstr\nNone\nString of loss or funciton which defines the loss. This amounts to Loss function that is used for Gradient Boosting models to calculate impurity.\n\n\nverbose\ntyping.Union[bool, int]\nFalse\nVerbosity for tree builidng\n\n\ndigits\ntyping.Union[int, NoneType]\n2\nTo round the values before doing a split"
  },
  {
    "objectID": "01_APIS/base.data.html",
    "href": "01_APIS/base.data.html",
    "title": "Dataset",
    "section": "",
    "text": "source\n\nDataset\n\n Dataset (df:pandas.core.frame.DataFrame, reduce_datatype:bool=True,\n          encode_category:str=None, add_intercept:bool=False,\n          na_treatment:str='allow', copy_data:bool=False, digits:int=None,\n          n_category:Union[int,float,NoneType]=None)\n\nDataset Adaptor Class\nThis class is meant to make dataset possible which would be consumed by models further\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\nDataframe that needs to be converted\n\n\nreduce_datatype\nbool\nTrue\nShall we try to reduce datatype to make is smaller\n\n\nencode_category\nstr\nNone\nDo encoding of categories default to None as no encoding\n\n\nadd_intercept\nbool\nFalse\nAdd a constant value intercept to data. This might be needed for Model based Trees.\n\n\nna_treatment\nstr\nallow\nHow to work with nas. Default: ‘allow’\n\n\ncopy_data\nbool\nFalse\nKeep a self copy of original data\n\n\ndigits\nint\nNone\nTo round float to certain digits or not, Default: None means no rounding\n\n\nn_category\ntyping.Union[int, float, NoneType]\nNone\nHow many different level shoud be treated as category. If a value less than one the number of levels is defined aas % oft total rows\n\n\nReturns\nNone\n\n\n\n\n\n\n\nHow to work on data\nPlease refer Examples"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mltreelib : Machine Learnign with Tree Based Library",
    "section": "",
    "text": "This package evovled from the attempt to make right kind of Decision Tress which was ideated by many people like Hastie, Tibshirani, Friedman, Quilan, Loh, Chaudhari."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "mltreelib : Machine Learnign with Tree Based Library",
    "section": "Install",
    "text": "Install\npip install mltreelib"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "mltreelib : Machine Learnign with Tree Based Library",
    "section": "How to use",
    "text": "How to use\nCreate a sample data\n\nimport numpy as np\nimport pandas as pd\nfrom mltreelib.data import Dataset\nfrom mltreelib.tree import Tree\n\n\nn_size = 1000\nrnd = np.random.RandomState(1234)\ndummy_data = pd.DataFrame({'numericfull':rnd.randint(1,500,size=n_size),\n                            'unitint':rnd.randint(1,25,size=n_size),\n                            'floatfull':rnd.random_sample(size=n_size),\n                            'floatsmall':np.round(rnd.random_sample(size=n_size)+rnd.randint(1,25,size=n_size),2),\n                            'categoryobj':rnd.choice(['a','b','c','d'],size=n_size),\n                            'stringobj':rnd.choice([\"{:c}\".format(k) for k in range(97, 123)],size=n_size)})\ndummy_data.head()\n\n\n\n\n\n  \n    \n      \n      numericfull\n      unitint\n      floatfull\n      floatsmall\n      categoryobj\n      stringobj\n    \n  \n  \n    \n      0\n      304\n      18\n      0.908959\n      8.56\n      a\n      c\n    \n    \n      1\n      212\n      24\n      0.348582\n      14.35\n      a\n      g\n    \n    \n      2\n      295\n      15\n      0.392977\n      21.98\n      a\n      y\n    \n    \n      3\n      54\n      20\n      0.720856\n      5.33\n      a\n      q\n    \n    \n      4\n      205\n      21\n      0.897588\n      23.03\n      c\n      k\n    \n  \n\n\n\n\nCreate a Dataset\n\ndataset = Dataset(df=dummy_data)\nprint(dataset)\nprint('Pandas Data Frame        : ',np.round(dummy_data.memory_usage(deep=True).sum()*1e-6,2),'MB')\nprint('Dataset Structured Array : ',np.round(dataset.data.nbytes*1e-6/ 1024 * 1024,2),'MB')\ndataset.data[:5]\n\nDataset(df=Shape((1000, 6), reduce_datatype=True, encode_category=None, add_intercept=False, na_treatment=allow, copy=False, digits=None, n_category=None, split_ratio=None)\nPandas Data Frame        :  0.15 MB\nDataset Structured Array :  0.03 MB\n\n\narray([(304, 18, 0.9089594 ,  8.56, 'a', 'c'),\n       (212, 24, 0.34858167, 14.35, 'a', 'g'),\n       (295, 15, 0.39297667, 21.98, 'a', 'y'),\n       ( 54, 20, 0.7208556 ,  5.33, 'a', 'q'),\n       (205, 21, 0.89758754, 23.03, 'c', 'k')],\n      dtype=[('numericfull', '<u2'), ('unitint', 'u1'), ('floatfull', '<f4'), ('floatsmall', '<f4'), ('categoryobj', 'O'), ('stringobj', 'O')])"
  }
]