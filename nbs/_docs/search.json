[
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utility functions",
    "section": "",
    "text": "source\n\nrbf_kernel\n\n rbf_kernel (gamma, **kwargs)\n\n\nsource\n\n\npolynomial_kernel\n\n polynomial_kernel (power, coef, **kwargs)\n\n\nsource\n\n\nlinear_kernel\n\n linear_kernel (**kwargs)\n\n\nsource\n\n\ncalculate_correlation_matrix\n\n calculate_correlation_matrix (X, Y=None)\n\nCalculate the correlation matrix for the dataset X\n\nsource\n\n\ncalculate_covariance_matrix\n\n calculate_covariance_matrix (X, Y=None)\n\nCalculate the covariance matrix for the dataset X\n\nsource\n\n\naccuracy_score\n\n accuracy_score (y_true, y_pred)\n\nCompare y_true to y_pred and return the accuracy\n\nsource\n\n\neuclidean_distance\n\n euclidean_distance (x1, x2)\n\nCalculates the l2 distance between two vectors\n\nsource\n\n\ncalculate_std_dev\n\n calculate_std_dev (X)\n\nCalculate the standard deviations of the features in dataset X\n\nsource\n\n\ncalculate_variance\n\n calculate_variance (X)\n\nReturn the variance of the features in dataset X\n\nsource\n\n\nmean_squared_error\n\n mean_squared_error (y_true, y_pred)\n\nReturns the mean squared error between y_true and y_pred\n\nsource\n\n\ncalculate_entropy\n\n calculate_entropy (y)\n\nCalculate the entropy of label array y"
  },
  {
    "objectID": "basetree.html",
    "href": "basetree.html",
    "title": "BaseTree",
    "section": "",
    "text": "source\n\nBaseDecisionTree\n\n BaseDecisionTree (min_samples_split=2, min_impurity=1e-07, max_depth=inf,\n                   loss=None, verbose=False, round_float=True,\n                   round_digit=4)\n\nSuper class of RegressionTree and ClassificationTree. Parameters: ———– min_samples_split: int The minimum number of samples needed to make a split when building a tree. min_impurity: float The minimum impurity required to split the tree further. max_depth: int The maximum depth of a tree. loss: function Loss function that is used for Gradient Boosting models to calculate impurity.\n\nsource\n\n\nDecisionNode\n\n DecisionNode (feature_i=None, threshold=None, value=None,\n               true_branch=None, false_branch=None, depth=None)\n\nClass that represents a decision node or leaf in the decision tree Parameters: ———– feature_i: int Feature index which we want to use as the threshold measure. threshold: float The value that we will compare feature values at feature_i against to determine the prediction. value: float The class prediction if classification tree, or float value if regression tree. true_branch: DecisionNode Next decision node for samples where features value met the threshold. false_branch: DecisionNode Next decision node for samples where features value did not meet the threshold.\n\nsource\n\n\nget_sorted_cats\n\n get_sorted_cats (x, ascending=True)\n\n\nsource\n\n\ndivide_on_feature\n\n divide_on_feature (X, feature_i, threshold)\n\nDivide dataset based on if sample value on feature index is larger than the given threshold"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mltree",
    "section": "",
    "text": "This package evovled from the attempt to make right kind of Decision Tress which was ideated by many people like Hastie, Tibshirani, Friedman, Quilan, Loh, Chaudhari."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "mltree",
    "section": "Install",
    "text": "Install\npip install mltree"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "mltree",
    "section": "How to use",
    "text": "How to use\nCreate a sample data\n\nimport numpy as np\nimport pandas as pd\nfrom mltree.data import Dataset\nfrom mltree.tree import Tree\n\n2\n\n\n\nn_size = 1000\nrnd = np.random.RandomState(1234)\ndummy_data = pd.DataFrame({'numericfull':rnd.randint(1,500,size=n_size),\n                            'unitint':rnd.randint(1,25,size=n_size),\n                            'floatfull':rnd.random_sample(size=n_size),\n                            'floatsmall':np.round(rnd.random_sample(size=n_size)+rnd.randint(1,25,size=n_size),2),\n                            'categoryobj':rnd.choice(['a','b','c','d'],size=n_size),\n                            'stringobj':rnd.choice([\"{:c}\".format(k) for k in range(97, 123)],size=n_size)})\ndummy_data.head()\n\nCreate a Dataset\n\ndataset = Dataset(df=dummy_data)\nprint(dataset)\nprint('Pandas Data Frame        : ',np.round(dummy_data.memory_usage(deep=True).sum()*1e-6,2),'MB')\nprint('Dataset Structured Array : ',np.round(dataset.data.nbytes*1e-6/ 1024 * 1024,2),'MB')\ndataset.data[:5]"
  },
  {
    "objectID": "tree.html",
    "href": "tree.html",
    "title": "Tree Implementations",
    "section": "",
    "text": "source\n\nTree\n\n Tree ()\n\nTree class enable building all simpe single tree models\n\nsource\n\n\nCARTRegressionTree\n\n CARTRegressionTree (min_samples_split=2, min_impurity=1e-07,\n                     max_depth=inf, loss=None, verbose=False,\n                     round_float=True, round_digit=4)\n\nSuper class of RegressionTree and ClassificationTree. Parameters: ———– min_samples_split: int The minimum number of samples needed to make a split when building a tree. min_impurity: float The minimum impurity required to split the tree further. max_depth: int The maximum depth of a tree. loss: function Loss function that is used for Gradient Boosting models to calculate impurity."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Dataset",
    "section": "",
    "text": "source\n\nDataset\n\n Dataset (df:pandas.core.frame.DataFrame, reduce_datatype:bool=True,\n          encode_category:str=None, add_intercept:bool=False,\n          na_treatment:str='allow', copy_data:bool=False, digits:int=None,\n          n_category:Union[int,float,NoneType]=None)\n\nDataset Adaptor Class\nThis class is meant to make dataset possible which would be consumed by models further\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\nDataframe that needs to be converted\n\n\nreduce_datatype\nbool\nTrue\nShall we try to reduce datatype to make is smaller\n\n\nencode_category\nstr\nNone\nDo encoding of categories default to None as no encoding\n\n\nadd_intercept\nbool\nFalse\nAdd a constant value intercept to data. This might be needed for Model based Trees.\n\n\nna_treatment\nstr\nallow\nHow to work with nas. Default: ‘allow’\n\n\ncopy_data\nbool\nFalse\nKeep a self copy of original data\n\n\ndigits\nint\nNone\nTo round float to certain digits or not, Default: None means no rounding\n\n\nn_category\ntyping.Union[int, float, NoneType]\nNone\nHow many different level shoud be treated as category. If a value less than one the number of levels is defined aas % oft total rows\n\n\nReturns\nNone\n\n\n\n\n\n\n\nHow to work on data\nCreating random sample data:\n\nn_size = 1000000\nrnd = np.random.RandomState(1234)\ndummy_data = pd.DataFrame({'numericfull':rnd.randint(1,500,size=n_size),\n                            'unitint':rnd.randint(1,25,size=n_size),\n                            'floatfull':rnd.random_sample(size=n_size),\n                            'floatsmall':np.round(rnd.random_sample(size=n_size)+rnd.randint(1,25,size=n_size),2),\n                            'categoryobj':rnd.choice(['a','b','c','d'],size=n_size),\n                            'stringobj':rnd.choice([\"{:c}\".format(k) for k in range(97, 123)],size=n_size)})\ndummy_data.head()\n\n\n\n\n\n  \n    \n      \n      numericfull\n      unitint\n      floatfull\n      floatsmall\n      categoryobj\n      stringobj\n    \n  \n  \n    \n      0\n      304\n      1\n      0.651859\n      11.42\n      a\n      f\n    \n    \n      1\n      212\n      1\n      0.906869\n      23.28\n      d\n      v\n    \n    \n      2\n      295\n      23\n      0.933262\n      21.79\n      d\n      t\n    \n    \n      3\n      54\n      19\n      0.919103\n      9.24\n      d\n      s\n    \n    \n      4\n      205\n      9\n      0.262066\n      16.69\n      a\n      l\n    \n  \n\n\n\n\nPass it to Dataset and let it do its magic\n\ndataset = Dataset(df=dummy_data)\ndataset\n\nDataset(df=Shape((1000000, 6), reduce_datatype=True, encode_category=None, add_intercept=False, na_treatment=allow, copy=False, digits=None, n_category=None, split_ratio=None)\n\n\nTo acess raw processed data\n\ndataset.data[:5]\n\narray([(304,  1, 0.65185905, 11.42, 'a', 'f'),\n       (212,  1, 0.90686905, 23.28, 'd', 'v'),\n       (295, 23, 0.9332624 , 21.79, 'd', 't'),\n       ( 54, 19, 0.9191031 ,  9.24, 'd', 's'),\n       (205,  9, 0.2620663 , 16.69, 'a', 'l')],\n      dtype=[('numericfull', '<u2'), ('unitint', 'u1'), ('floatfull', '<f4'), ('floatsmall', '<f4'), ('categoryobj', 'O'), ('stringobj', 'O')])\n\n\nNote: This is a Structured arrays and not a simmple numpy array or pandas data frame.\nSize reduction is as follows:\n\nprint('Pandas Data Frame        : ',np.round(dummy_data.memory_usage(deep=True).sum()*1e-6,2),'MB')\nprint('Dataset Structured Array : ',np.round(dataset.data.nbytes*1e-6/ 1024 * 1024,2),'MB')\n\nPandas Data Frame        :  148.0 MB\nDataset Structured Array :  27.0 MB\n\n\n\nprint(dummy_data.info(memory_usage='deep'))\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 6 columns):\n #   Column       Non-Null Count    Dtype  \n---  ------       --------------    -----  \n 0   numericfull  1000000 non-null  int64  \n 1   unitint      1000000 non-null  int64  \n 2   floatfull    1000000 non-null  float64\n 3   floatsmall   1000000 non-null  float64\n 4   categoryobj  1000000 non-null  object \n 5   stringobj    1000000 non-null  object \ndtypes: float64(2), int64(2), object(2)\nmemory usage: 141.1 MB\nNone\n\n\n\n\nFurther reduction in data size\nWe can even further reduce data by using following parameters:\n\ndataset = Dataset(df=dummy_data, digits=2)\nprint('Pandas Data Frame        : ',np.round(dummy_data.memory_usage(deep=True).sum()*1e-6,2),'MB')\nprint('Dataset Structured Array : ',np.round(dataset.data.nbytes*1e-6/ 1024 * 1024,2),'MB')\n\nPandas Data Frame        :  148.0 MB\nDataset Structured Array :  27.0 MB\n\n\n\ndataset.data[:5]\n\narray([(304,  1, 0.65, 11.42, 'a', 'f'), (212,  1, 0.91, 23.28, 'd', 'v'),\n       (295, 23, 0.93, 21.79, 'd', 't'), ( 54, 19, 0.92,  9.24, 'd', 's'),\n       (205,  9, 0.26, 16.69, 'a', 'l')],\n      dtype=[('numericfull', '<u2'), ('unitint', 'u1'), ('floatfull', '<f4'), ('floatsmall', '<f4'), ('categoryobj', 'O'), ('stringobj', 'O')])\n\n\n\n# data.features_\n# dummy_data.to_dict?\n# data._dtypes = data.df.dtypes.to_dict()\n# data._dtypes\n# dummy_data.rename?\n# {k:v==object for k,v in data._dtypes.items()"
  }
]