[
  {
    "objectID": "00_Examples/core.html",
    "href": "00_Examples/core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  },
  {
    "objectID": "00_Examples/00_dataset.html",
    "href": "00_Examples/00_dataset.html",
    "title": "How to work with Datasets",
    "section": "",
    "text": "n_size = 1000000\nrnd = np.random.RandomState(1234)\ndummy_data = pd.DataFrame({\n    'numericfull':rnd.randint(1,500,size=n_size),\n    'unitint':rnd.randint(1,25,size=n_size),\n    'floatfull':rnd.random_sample(size=n_size),\n    'floatsmall':np.round(rnd.random_sample(size=n_size)+rnd.randint(1,25,size=n_size),2),\n    'categoryobj':rnd.choice(['a','b','c','d'],size=n_size),\n    'stringobj':rnd.choice([\"{:c}\".format(k) for k in range(97, 123)],size=n_size)})\ndummy_data.head()\n\n\n\n\n\n  \n    \n      \n      numericfull\n      unitint\n      floatfull\n      floatsmall\n      categoryobj\n      stringobj\n    \n  \n  \n    \n      0\n      304\n      1\n      0.651859\n      11.42\n      a\n      f\n    \n    \n      1\n      212\n      1\n      0.906869\n      23.28\n      d\n      v\n    \n    \n      2\n      295\n      23\n      0.933262\n      21.79\n      d\n      t\n    \n    \n      3\n      54\n      19\n      0.919103\n      9.24\n      d\n      s\n    \n    \n      4\n      205\n      9\n      0.262066\n      16.69\n      a\n      l\n    \n  \n\n\n\n\nPass it to Dataset and let it do its magic\n\ndataset = Data(df=dummy_data)\ndataset\n\nDataset(df=Shape((1000000, 6), reduce_datatype=True, encode_category=None, add_intercept=False, na_treatment=allow, copy=False, digits=None, n_category=None, split_ratio=None)\n\n\nTo acess raw processed data\n\ndataset.data[:5]\n\narray([(304,  1, 0.65185905, 11.42, 'a', 'f'),\n       (212,  1, 0.90686905, 23.28, 'd', 'v'),\n       (295, 23, 0.9332624 , 21.79, 'd', 't'),\n       ( 54, 19, 0.9191031 ,  9.24, 'd', 's'),\n       (205,  9, 0.2620663 , 16.69, 'a', 'l')],\n      dtype=[('numericfull', '<u2'), ('unitint', 'u1'), ('floatfull', '<f4'), ('floatsmall', '<f4'), ('categoryobj', 'O'), ('stringobj', 'O')])\n\n\nNote: This is a Structured arrays and not a simmple numpy array or pandas data frame.\nSize reduction is as follows:\n\nprint('Pandas Data Frame        : ',np.round(dummy_data.memory_usage(deep=True).sum()*1e-6,2),'MB')\nprint('Dataset Structured Array : ',np.round(dataset.data.nbytes*1e-6/ 1024 * 1024,2),'MB')\n\nPandas Data Frame        :  148.0 MB\nDataset Structured Array :  27.0 MB\n\n\n\nprint(dummy_data.info(memory_usage='deep'))\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000000 entries, 0 to 999999\nData columns (total 6 columns):\n #   Column       Non-Null Count    Dtype  \n---  ------       --------------    -----  \n 0   numericfull  1000000 non-null  int64  \n 1   unitint      1000000 non-null  int64  \n 2   floatfull    1000000 non-null  float64\n 3   floatsmall   1000000 non-null  float64\n 4   categoryobj  1000000 non-null  object \n 5   stringobj    1000000 non-null  object \ndtypes: float64(2), int64(2), object(2)\nmemory usage: 141.1 MB\nNone\n\n\n\nFurther reduction in data size\nWe can even further reduce data by using following parameters:\n\ndataset = Data(df=dummy_data, digits=2)\nprint('Pandas Data Frame        : ',np.round(dummy_data.memory_usage(deep=True).sum()*1e-6,2),'MB')\nprint('Dataset Structured Array : ',np.round(dataset.data.nbytes*1e-6/ 1024 * 1024,2),'MB')\n\nPandas Data Frame        :  148.0 MB\nDataset Structured Array :  27.0 MB\n\n\n\ndataset.data[:5]\n\narray([(304,  1, 0.65, 11.42, 'a', 'f'), (212,  1, 0.91, 23.28, 'd', 'v'),\n       (295, 23, 0.93, 21.79, 'd', 't'), ( 54, 19, 0.92,  9.24, 'd', 's'),\n       (205,  9, 0.26, 16.69, 'a', 'l')],\n      dtype=[('numericfull', '<u2'), ('unitint', 'u1'), ('floatfull', '<f4'), ('floatsmall', '<f4'), ('categoryobj', 'O'), ('stringobj', 'O')])"
  },
  {
    "objectID": "01_APIS/data.utility.html",
    "href": "01_APIS/data.utility.html",
    "title": "Some classes to make life bit easy",
    "section": "",
    "text": "source\n\nIndexMapper\n\n IndexMapper (categorical_columns:Union[str,list], verbose:bool=False)\n\nA multi column categorical labeller :param categorical_columns: :param verbose: Verbosity\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncategorical_columns\ntyping.Union[str, list]\n\nColumns to create Label for\n\n\nverbose\nbool\nFalse\nVerbosity\n\n\n\n\nsource\n\n\nVariableTransformation\n\n VariableTransformation (variables:Union[str,list], strategy='zscore',\n                         custom_func=None)\n\nVariable Transformation for all features independently\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvariables\ntyping.Union[str, list]\n\nVariables for which we need to perform the transformation\n\n\nstrategy\nstr\nzscore\nstrategy on how to transform variables ‘zscore’,‘min-max’,‘median-iqr’, mean-iqr’,‘median-qd’, ‘median-std’,‘median-range’,‘mean-range’,‘min-qd’,‘min-iqr’, ‘min’,‘max’,‘mean’,‘median’,‘std’,‘iqr’,‘qd’,‘range’,‘custom’\n\n\ncustom_func\nNoneType\nNone\nThe function should take 4 arguments custom_func(value,group,groups_estimates,inverse)\n\n\n\n\nsource\n\n\nGroupedVariableTransformation\n\n GroupedVariableTransformation (key, target, monotone_constraints=None,\n                                strategy='zscore', custom_func=None)\n\nVariable Transformation at group level\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nkey\n\n\nGrouping / ID column against which aggregation would happen\n\n\ntarget\n\n\nTarget column name which needs to be used for creating aggregation\n\n\nmonotone_constraints\nNoneType\nNone\nMonotonic Constraints\n\n\nstrategy\nstr\nzscore\nstrategy on how to transform variables ‘zscore’,‘min-max’,‘median-iqr’, mean-iqr’,‘median-qd’, ‘median-std’,‘median-range’,‘mean-range’,‘min-qd’,‘min-iqr’, ‘min’,‘max’,‘mean’,‘median’,‘std’,‘iqr’,‘qd’,‘range’,‘custom’\n\n\ncustom_func\nNoneType\nNone\nA custom function of form that take 4 arguments : custom_func(value,group,groups_estimates,inverse)"
  },
  {
    "objectID": "01_APIS/utils.metrics.html",
    "href": "01_APIS/utils.metrics.html",
    "title": "Metrics",
    "section": "",
    "text": "source\n\naccuracy_score\n\n accuracy_score (y_true, y_pred)\n\nCompare y_true to y_pred and return the accuracy\n\nsource\n\n\nmae\n\n mae (y_true, y_pred)\n\nReturns the mean squared error between y_true and y_pred\n\nsource\n\n\nmse\n\n mse (y_true, y_pred)\n\nReturns the mean squared error between y_true and y_pred"
  },
  {
    "objectID": "01_APIS/tree.models.html",
    "href": "01_APIS/tree.models.html",
    "title": "Tree Implementations",
    "section": "",
    "text": "source\n\nTree\n\n Tree ()\n\nTree class enable building all simpe single tree models\n\nsource\n\n\nCARTRegressionTree\n\n CARTRegressionTree (objective:str='class', min_samples_split:int=20,\n                     min_sample_leaf:int=10, max_compete:int=4,\n                     min_impurity:float=0.01, max_depth:int=inf,\n                     max_surrogates:int=2, surrogate_style:int=2,\n                     parallelize:str='feature', tree_growth:str='cart',\n                     feature_weights:Union[float,int,list]=None,\n                     loss:Union[str,float]=None,\n                     verbose:Union[bool,int]=False,\n                     digits:Union[int,NoneType]=2)\n\nSuper class of all kinds of tree.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nobjective\nstr\nclass\ndefining objective of the whole tree building.\n\n\nmin_samples_split\nint\n20\nThe minimum number of samples needed to make a split when building a tree.\n\n\nmin_sample_leaf\nint\n10\nMinimum sample required to have a leaf node\n\n\nmax_compete\nint\n4\nthe number of competitor splits retained in the output. It is useful to know not just which split was chosen, but which variable came in second, third, etc.\n\n\nmin_impurity\nfloat\n0.01\nThe minimum impurity required to split the tree further. this is equivalent to complexity in CART\n\n\nmax_depth\nint\ninf\nThe maximum depth of a tree.\n\n\nmax_surrogates\nint\n2\nmore on this to make it generalized and not CART Specific\n\n\nsurrogate_style\nint\n2\nmore on this to make it generalized and not CART Specific\n\n\nparallelize\nstr\nfeature\nfollow form LightGBMs behavior\n\n\ntree_growth\nstr\ncart\nfollow binary structure if cart else follow multiple structure as C4.5\n\n\nfeature_weights\ntyping.Union[float, int, list]\nNone\nweight of each feature in the split. Default is set to 1 for all features\n\n\nloss\ntyping.Union[str, float]\nNone\nString of loss or function which defines the loss. This amounts to Loss function that is used for Gradient Boosting models to calculate impurity.\n\n\nverbose\ntyping.Union[bool, int]\nFalse\nVerbosity for tree building\n\n\ndigits\ntyping.Union[int, NoneType]\n2\nTo round the values before doing a split\n\n\n\n\nsource\n\n\nCART\n\n CART (objective:str='class', min_samples_split:int=20,\n       min_sample_leaf:int=10, max_compete:int=4, min_impurity:float=0.01,\n       max_depth:int=inf, max_surrogates:int=2, surrogate_style:int=2,\n       parallelize:str='feature', tree_growth:str='cart',\n       feature_weights:Union[float,int,list]=None,\n       loss:Union[str,float]=None, verbose:Union[bool,int]=False,\n       digits:Union[int,NoneType]=2)\n\nSuper class of all kinds of tree.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nobjective\nstr\nclass\ndefining objective of the whole tree building.\n\n\nmin_samples_split\nint\n20\nThe minimum number of samples needed to make a split when building a tree.\n\n\nmin_sample_leaf\nint\n10\nMinimum sample required to have a leaf node\n\n\nmax_compete\nint\n4\nthe number of competitor splits retained in the output. It is useful to know not just which split was chosen, but which variable came in second, third, etc.\n\n\nmin_impurity\nfloat\n0.01\nThe minimum impurity required to split the tree further. this is equivalent to complexity in CART\n\n\nmax_depth\nint\ninf\nThe maximum depth of a tree.\n\n\nmax_surrogates\nint\n2\nmore on this to make it generalized and not CART Specific\n\n\nsurrogate_style\nint\n2\nmore on this to make it generalized and not CART Specific\n\n\nparallelize\nstr\nfeature\nfollow form LightGBMs behavior\n\n\ntree_growth\nstr\ncart\nfollow binary structure if cart else follow multiple structure as C4.5\n\n\nfeature_weights\ntyping.Union[float, int, list]\nNone\nweight of each feature in the split. Default is set to 1 for all features\n\n\nloss\ntyping.Union[str, float]\nNone\nString of loss or function which defines the loss. This amounts to Loss function that is used for Gradient Boosting models to calculate impurity.\n\n\nverbose\ntyping.Union[bool, int]\nFalse\nVerbosity for tree building\n\n\ndigits\ntyping.Union[int, NoneType]\n2\nTo round the values before doing a split\n\n\n\n\nsource\n\n\nC45\n\n C45 (objective:str='class', min_samples_split:int=20,\n      min_sample_leaf:int=10, max_compete:int=4, min_impurity:float=0.01,\n      max_depth:int=inf, max_surrogates:int=2, surrogate_style:int=2,\n      parallelize:str='feature', tree_growth:str='cart',\n      feature_weights:Union[float,int,list]=None,\n      loss:Union[str,float]=None, verbose:Union[bool,int]=False,\n      digits:Union[int,NoneType]=2)\n\nSuper class of all kinds of tree.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nobjective\nstr\nclass\ndefining objective of the whole tree building.\n\n\nmin_samples_split\nint\n20\nThe minimum number of samples needed to make a split when building a tree.\n\n\nmin_sample_leaf\nint\n10\nMinimum sample required to have a leaf node\n\n\nmax_compete\nint\n4\nthe number of competitor splits retained in the output. It is useful to know not just which split was chosen, but which variable came in second, third, etc.\n\n\nmin_impurity\nfloat\n0.01\nThe minimum impurity required to split the tree further. this is equivalent to complexity in CART\n\n\nmax_depth\nint\ninf\nThe maximum depth of a tree.\n\n\nmax_surrogates\nint\n2\nmore on this to make it generalized and not CART Specific\n\n\nsurrogate_style\nint\n2\nmore on this to make it generalized and not CART Specific\n\n\nparallelize\nstr\nfeature\nfollow form LightGBMs behavior\n\n\ntree_growth\nstr\ncart\nfollow binary structure if cart else follow multiple structure as C4.5\n\n\nfeature_weights\ntyping.Union[float, int, list]\nNone\nweight of each feature in the split. Default is set to 1 for all features\n\n\nloss\ntyping.Union[str, float]\nNone\nString of loss or function which defines the loss. This amounts to Loss function that is used for Gradient Boosting models to calculate impurity.\n\n\nverbose\ntyping.Union[bool, int]\nFalse\nVerbosity for tree building\n\n\ndigits\ntyping.Union[int, NoneType]\n2\nTo round the values before doing a split"
  },
  {
    "objectID": "01_APIS/data.datasets.html",
    "href": "01_APIS/data.datasets.html",
    "title": "Machine Learning Data Sets",
    "section": "",
    "text": "source\n\nshow_data\n\n show_data ()\n\nLists all the data available with the package\n\nsource\n\n\nget_dataset\n\n get_dataset (task='regression', data_of='palmerpenguins',\n              description=True, return_single_df=True, target_name=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntask\nstr\nregression\ndefine the need of task\n\n\ndata_of\nstr\npalmerpenguins\nexpress which data is needed\n\n\ndescription\nbool\nTrue\nweather pass description as and added output\n\n\nreturn_single_df\nbool\nTrue\nreturn a single data frame or provide X,y behavior as with scikit-learn.\n\n\ntarget_name\nNoneType\nNone\nif target name is none the original mentioned target name would be used.\n\n\n\n\nshow_data()\n\nCurrently available datasets are:\n['adult', 'palmerpenguins-raw', 'palmerpenguins', 'california', 'titanic']\n\n\n\n# get_dataset(data_of='titanic').head()\n\n\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0.0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1.0\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1.0\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1.0\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0.0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S\n    \n  \n\n\n\n\n\n# get_dataset(data_of='california').head()\n\n\n\n\n\n  \n    \n      \n      longitude\n      latitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      median_house_value\n      ocean_proximity\n    \n  \n  \n    \n      0\n      -122.23\n      37.88\n      41.0\n      880.0\n      129.0\n      322.0\n      126.0\n      8.3252\n      452600.0\n      NEAR BAY\n    \n    \n      1\n      -122.22\n      37.86\n      21.0\n      7099.0\n      1106.0\n      2401.0\n      1138.0\n      8.3014\n      358500.0\n      NEAR BAY\n    \n    \n      2\n      -122.24\n      37.85\n      52.0\n      1467.0\n      190.0\n      496.0\n      177.0\n      7.2574\n      352100.0\n      NEAR BAY\n    \n    \n      3\n      -122.25\n      37.85\n      52.0\n      1274.0\n      235.0\n      558.0\n      219.0\n      5.6431\n      341300.0\n      NEAR BAY\n    \n    \n      4\n      -122.25\n      37.85\n      52.0\n      1627.0\n      280.0\n      565.0\n      259.0\n      3.8462\n      342200.0\n      NEAR BAY\n    \n  \n\n\n\n\n\n# get_dataset(data_of='palmerpenguins-raw').head()\n\nIndex(['studyName', 'Sample Number', 'Species', 'Region', 'Island', 'Stage',\n       'Individual ID', 'Clutch Completion', 'Date Egg', 'Culmen Length (mm)',\n       'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Sex',\n       'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)', 'Comments'],\n      dtype='object')\n\n\n\n# get_dataset(data_of='palmerpenguins').head()\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie Penguin (Pygoscelis adeliae)\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      MALE\n      2007-11-11\n    \n    \n      1\n      Adelie Penguin (Pygoscelis adeliae)\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      FEMALE\n      2007-11-11\n    \n    \n      2\n      Adelie Penguin (Pygoscelis adeliae)\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      FEMALE\n      2007-11-16\n    \n    \n      3\n      Adelie Penguin (Pygoscelis adeliae)\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      2007-11-16\n    \n    \n      4\n      Adelie Penguin (Pygoscelis adeliae)\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      FEMALE\n      2007-11-16\n    \n  \n\n\n\n\n\n# get_dataset(data_of='adult').head()\n\n\n\n\n\n  \n    \n      \n      age\n      workclass\n      fnlwgt\n      education\n      education_num\n      marital_status\n      occupation\n      relationship\n      race\n      sex\n      capital_gain\n      capital_loss\n      hours_per_week\n      native_country\n      target\n    \n  \n  \n    \n      0\n      39\n      State-gov\n      77516\n      Bachelors\n      13\n      Never-married\n      Adm-clerical\n      Not-in-family\n      White\n      Male\n      2174\n      0\n      40\n      United-States\n      <=50K\n    \n    \n      1\n      50\n      Self-emp-not-inc\n      83311\n      Bachelors\n      13\n      Married-civ-spouse\n      Exec-managerial\n      Husband\n      White\n      Male\n      0\n      0\n      13\n      United-States\n      <=50K\n    \n    \n      2\n      38\n      Private\n      215646\n      HS-grad\n      9\n      Divorced\n      Handlers-cleaners\n      Not-in-family\n      White\n      Male\n      0\n      0\n      40\n      United-States\n      <=50K\n    \n    \n      3\n      53\n      Private\n      234721\n      11th\n      7\n      Married-civ-spouse\n      Handlers-cleaners\n      Husband\n      Black\n      Male\n      0\n      0\n      40\n      United-States\n      <=50K\n    \n    \n      4\n      28\n      Private\n      338409\n      Bachelors\n      13\n      Married-civ-spouse\n      Prof-specialty\n      Wife\n      Black\n      Female\n      0\n      0\n      40\n      Cuba\n      <=50K"
  },
  {
    "objectID": "01_APIS/data.data.html",
    "href": "01_APIS/data.data.html",
    "title": "Dataset",
    "section": "",
    "text": "source\n\nData\n\n Data (df:pandas.core.frame.DataFrame, reduce_datatype:bool=True,\n       encode_category:str=None, add_intercept:bool=False,\n       na_treatment:str='allow', copy_data:bool=False, digits:int=None,\n       n_category:Union[int,float,NoneType]=None)\n\nDataset Adaptor Class\nThis class is meant to make dataset possible which would be consumed by models further\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\nDataframe that needs to be converted\n\n\nreduce_datatype\nbool\nTrue\nShall we try to reduce datatype to make is smaller\n\n\nencode_category\nstr\nNone\nDo encoding of categories default to None as no encoding\n\n\nadd_intercept\nbool\nFalse\nAdd a constant value intercept to data. This might be needed for Model based Trees.\n\n\nna_treatment\nstr\nallow\nHow to work with nas. Default: ‘allow’\n\n\ncopy_data\nbool\nFalse\nKeep a self copy of original data\n\n\ndigits\nint\nNone\nTo round float to certain digits or not, Default: None means no rounding\n\n\nn_category\ntyping.Union[int, float, NoneType]\nNone\nHow many different level shoud be treated as category. If a value less than one the number of levels is defined aas % oft total rows\n\n\nReturns\nNone\n\n\n\n\n\n\n\nHow to work on data\nPlease refer Examples"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mltreelib : Machine Learnign with Tree Based Library",
    "section": "",
    "text": "This package evovled from the attempt to make right kind of Decision Tress which was ideated by many people like Hastie, Tibshirani, Friedman, Quilan, Loh, Chaudhari."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "mltreelib : Machine Learnign with Tree Based Library",
    "section": "Install",
    "text": "Install\npip install mltreelib"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "mltreelib : Machine Learnign with Tree Based Library",
    "section": "How to use",
    "text": "How to use\nCreate a sample data\n\nimport numpy as np\nimport pandas as pd\nfrom mltreelib.data import Data\nfrom mltreelib.tree import Tree\n\n\nn_size = 1000\nrnd = np.random.RandomState(1234)\ndummy_data = pd.DataFrame({\n    'numericfull':rnd.randint(1,500,size=n_size),\n    'unitint':rnd.randint(1,25,size=n_size),\n    'floatfull':rnd.random_sample(size=n_size),\n    'floatsmall':np.round(rnd.random_sample(size=n_size)+rnd.randint(1,25,size=n_size),2),\n    'categoryobj':rnd.choice(['a','b','c','d'],size=n_size),\n    'stringobj':rnd.choice([\"{:c}\".format(k) for k in range(97, 123)],size=n_size)})\n    \ndummy_data.head()\n\n\n\n\n\n  \n    \n      \n      numericfull\n      unitint\n      floatfull\n      floatsmall\n      categoryobj\n      stringobj\n    \n  \n  \n    \n      0\n      304\n      18\n      0.908959\n      8.56\n      a\n      c\n    \n    \n      1\n      212\n      24\n      0.348582\n      14.35\n      a\n      g\n    \n    \n      2\n      295\n      15\n      0.392977\n      21.98\n      a\n      y\n    \n    \n      3\n      54\n      20\n      0.720856\n      5.33\n      a\n      q\n    \n    \n      4\n      205\n      21\n      0.897588\n      23.03\n      c\n      k\n    \n  \n\n\n\n\nCreate a Dataset\n\ndataset = Data(df=dummy_data)\nprint(dataset)\nprint('Pandas Data Frame        : ',np.round(dummy_data.memory_usage(deep=True).sum()*1e-6,2),'MB')\nprint('Dataset Structured Array : ',np.round(dataset.data.nbytes*1e-6/ 1024 * 1024,2),'MB')\ndataset.data[:5]\n\nDataset(df=Shape((1000, 6), reduce_datatype=True, encode_category=None, add_intercept=False, na_treatment=allow, copy=False, digits=None, n_category=None, split_ratio=None)\nPandas Data Frame        :  0.15 MB\nDataset Structured Array :  0.03 MB\n\n\narray([(304, 18, 0.9089594 ,  8.56, 'a', 'c'),\n       (212, 24, 0.34858167, 14.35, 'a', 'g'),\n       (295, 15, 0.39297667, 21.98, 'a', 'y'),\n       ( 54, 20, 0.7208556 ,  5.33, 'a', 'q'),\n       (205, 21, 0.89758754, 23.03, 'c', 'k')],\n      dtype=[('numericfull', '<u2'), ('unitint', 'u1'), ('floatfull', '<f4'), ('floatsmall', '<f4'), ('categoryobj', 'O'), ('stringobj', 'O')])"
  }
]