# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/01_APIS/03b_util_metrics.ipynb.

# %% auto 0
__all__ = ['mse', 'mae', 'accuracy_score']

# %% ../../nbs/01_APIS/03b_util_metrics.ipynb 3
from fastcore.utils import *
import numpy as np
import pandas as pd
import gc
import numba



# %% ../../nbs/01_APIS/03b_util_metrics.ipynb 5
def mse(y_true, y_pred):
    """ Returns the mean squared error between y_true and y_pred """
    mse = np.mean(np.power(y_true - y_pred, 2))
    return mse

def mae(y_true, y_pred):
    """ Returns the mean squared error between y_true and y_pred """
    mae = np.mean(np.abs(y_true - y_pred))
    return mae

def accuracy_score(y_true, y_pred):
    """ Compare y_true to y_pred and return the accuracy """
    accuracy = np.sum(y_true == y_pred, axis=0) / len(y_true)
    return accuracy


# To implement
# metric(s) to be evaluated on the evaluation set(s)
# "" (empty string or not specified) means that metric corresponding to specified objective will be used (this is possible only for pre-defined objective functions, otherwise no evaluation metric will be added)
# "None" (string, not a None value) means that no metric will be registered, aliases: na, null, custom
# l1, absolute loss, aliases: mean_absolute_error, mae, regression_l1
# l2, square loss, aliases: mean_squared_error, mse, regression_l2, regression
# nmse normalized mean square error
# mdape, median variant of these
# rmse, root square loss, aliases: root_mean_squared_error, l2_root
# quantile, Quantile regression
# mape, MAPE loss, aliases: mean_absolute_percentage_error
# huber, Huber loss
# fair, Fair loss
# poisson, negative log-likelihood for Poisson regression
# gamma, negative log-likelihood for Gamma regression
# gamma_deviance, residual deviance for Gamma regression
# tweedie, negative log-likelihood for Tweedie regression
# map, MAP, aliases: mean_average_precision
# auc, AUC
# average_precision, average precision score
# binary_logloss, log loss, aliases: binary
# binary_error, for one sample: 0 for correct classification, 1 for error classification
# auc_mu, AUC-mu
# multi_logloss, log loss for multi-class classification, aliases: multiclass, softmax, multiclassova, multiclass_ova, ova, ovr
# multi_error, error rate for multi-class classification
# cross_entropy, cross-entropy (with optional linear weights), aliases: xentropy
# cross_entropy_lambda, “intensity-weighted” cross-entropy, aliases: xentlambda
# kullback_leibler, Kullback-Leibler divergence, aliases: kldiv
