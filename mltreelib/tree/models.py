# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/01_APIS/03_trees_impl.ipynb.

# %% auto 0
__all__ = ['Tree', 'CARTRegressionTree', 'CART', 'C45']

# %% ../../nbs/01_APIS/03_trees_impl.ipynb 3
from fastcore.utils import *
import numpy as np
import pandas as pd
import gc
import numba
from ..base.tree import DecisionNode, BaseDecisionTree, Node
from ..utils.functions import _objectives,_losses



# %% ../../nbs/01_APIS/03_trees_impl.ipynb 5
class Tree:
    "Tree class enable building all simpe single tree models"

    def __init__(self) -> None:
        pass

    def fit_formula(self,formula,data):
        "Fit using formula and dataset"
        pass

    def fit(self,X,y):
        "Sklearn style fit"
        pass

    def fit_dataset(self,dataset,X_cols,target_names):
        "Fit in native way"
        pass

    def predict(self,X):
        "Best predicitons"
        pass

    def predict_leaf(self,X):
        "Predicting vlaues"
        pass

    def predict_proba(self,X):
        "Prediction probabilities"
        pass
    
    def transform(self,X,value_type='prob'):
        "Provide raw outputs form the tree"
        pass
    
    def apply(self,X):
        pass

    def describe(self, strucutre=False, plot=False, eval=False,importance=None):
        "Everythign to know about the tree"
        pass

    def prune(self,complexity):
        "Prune the tree based on certain criteria"
        pass

    def save(self):
        "save tree"
        pass
    def load(self):
        "load a tree"
        pass
    

# %% ../../nbs/01_APIS/03_trees_impl.ipynb 6
class CARTRegressionTree(BaseDecisionTree):
    
    def _calculate_variance_reduction(self, y, y1, y2):
        var_tot = calculate_variance(y)
        var_1 = calculate_variance(y1)
        var_2 = calculate_variance(y2)
        frac_1 = len(y1) / len(y)
        frac_2 = len(y2) / len(y)

        # Calculate the variance reduction
        variance_reduction = var_tot - (frac_1 * var_1 + frac_2 * var_2)

        return sum(variance_reduction)

    def _mean_of_y(self, y):
        value = np.mean(y, axis=0)
        if self.verbose: print('Leaf values:',value,value.shape,len(value.shape))
        return value
        # return value if len(value.shape) > 1 else value[0]
    
    def _cart_node_function(self,Xy, Xy1, Xy2, target,features):
        y = Xy[[target]].values
        y1 = Xy1[[target]].values
        y2 = Xy2[[target]].values
        impurity = self._calculate_variance_reduction(y,y1,y2)
        return impurity,None, None, None
    
    def _cart_leaf_function(self, Xy, target, features):
        mean_val = self._mean_of_y(Xy[target].values)
        return mean_val, None, None, None

    def fit(self, X, y):
        # output of both node function and leaf functionisin form leaf_value/impurity,estimator,weights,addons with inputs
        #inputs _node_function(Xy, Xy1,Xy2,target,features)
        self._node_function = self._cart_node_function
        #inputs _leaf_function(Xy, target, features)
        self._leaf_function = self._cart_leaf_function
        super(CARTRegressionTree, self).fit(X, y)



# %% ../../nbs/01_APIS/03_trees_impl.ipynb 7
class CART(BaseDecisionTree):
    
    def _calculate_impurity(self, y, y_l, y_r):
        
        #getting shapes
        n, n_l, n_r = len(y), len(y_l), len(y_r)
        
        if self._loss not in ['friedman_mse','chisq']:
            # defining loss function
            loss = _losses[self._loss]
            
            # evaluating losses
            parent_loss = loss(y)
            e_l, e_r = loss(y_l), loss(y_r)

            # impurity gain is difference in loss before vs. after split

            if self._loss in ['gini','entropy','var']:
                child_loss = (n_l / n) * e_l + (n_r / n) * e_r
                gain = np.sum(parent_loss - child_loss)
            elif self._loss in ['rss','mse','mae','std']:
                gain = e_l + el_r

        elif self._loss == 'friedman_mse':
            diff = np.mean(y_l) - np.mean(y_r)
            gain = n_l*n_r*diff*diff/(n_l+n_r)
        

        return gain

    # def _mean_of_y(self, y):
    #     value = np.mean(y, axis=0)
    #     if self.verbose: print('Leaf values:',value,value.shape,len(value.shape))
    #     return value
    #     # return value if len(value.shape) > 1 else value[0]
    
    def _cart_node_function(self,Xy, Xy1, Xy2, target, features):
        y = Xy[[target]].values
        y1 = Xy1[[target]].values
        y2 = Xy2[[target]].values
        impurity = self._calculate_impurity(y,y1,y2)
        return impurity,None, None, None
    
    def _cart_leaf_function(self, Xy, target, features):

        if self._objective == 'reg': 
            value = np.mean(Xy[target].values, axis=0)
            if self.verbose: print('Leaf values:',value,value.shape,len(value.shape))
            return value, None, None, None
            
        elif self._objective == 'class':
            values,counts = np.unique(Xy[target].values,return_counts=True)
            counts = counts/np.sum(counts)
            return values[np.argmax(counts)],None,{k:v for k,v in zip(values,counts)},None

    def fit(self, X, y):
        # output of both node function and leaf function is in form leaf_value/impurity,estimator,weights,addons with inputs
        #inputs _node_function(Xy, Xy1,Xy2,target,features)
        self._node_function = self._cart_node_function
        #inputs _leaf_function(Xy, target, features)
        self._leaf_function = self._cart_leaf_function
        super(CARTRegressionTree, self).fit(X, y)


# %% ../../nbs/01_APIS/03_trees_impl.ipynb 8
class C45(BaseDecisionTree):
    def _calculate_information_gain(self, y, y1, y2):
        # Calculate information gain
        p = len(y1) / len(y)
        entropy = calculate_entropy(y)
        info_gain = entropy - p * \
            calculate_entropy(y1) - (1 - p) * \
            calculate_entropy(y2)

        return info_gain

    def _majority_vote(self, y):
        most_common = None
        max_count = 0
        for label in np.unique(y):
            # Count number of occurences of samples with label
            count = len(y[y == label])
            if count > max_count:
                most_common = label
                max_count = count
        return most_common

    def fit(self, X, y):
        self._impurity_calculation = self._calculate_information_gain
        self._leaf_value_calculation = self._majority_vote
        super(C45, self).fit(X, y)
